{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project for AI 221\n",
    "\n",
    "AI 221 | 1st Sem AY 2024-2025  \n",
    "Students: Jemima Bian Anila, Joshua Cantor, Michael Spencer Quinto, MEng in AI  \n",
    "Instructors: Dr. Miguel Remolona | Dr. Jon Dewitt Dalisay\n",
    "\n",
    "## Dataset\n",
    "\n",
    "https://data.mendeley.com/datasets/tsvdyhbphs/1\n",
    "\n",
    "https://www.kaggle.com/datasets/jaydepaolomirandilla/philippine-medicinal-plant-leaf-dataset/data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "warnings.filterwarnings('ignore')\n",
    "np.set_printoptions(suppress=True, precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root directory to Python path to allow importing from src folder\n",
    "notebook_dir = Path(\"__file__\").resolve().parent\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import utility modules from `src` folder\n",
    "\n",
    "Note: all reusable code like transformations for the dataframes, etc. can be found inside the `src` folder and are imported here for better modularity and version control\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Inspection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_filename</th>\n",
       "      <th>class_name</th>\n",
       "      <th>class_number</th>\n",
       "      <th>rgb_0_mean</th>\n",
       "      <th>rgb_0_std</th>\n",
       "      <th>rgb_0_skewness</th>\n",
       "      <th>rgb_1_mean</th>\n",
       "      <th>rgb_1_std</th>\n",
       "      <th>rgb_1_skewness</th>\n",
       "      <th>rgb_2_mean</th>\n",
       "      <th>rgb_2_std</th>\n",
       "      <th>rgb_2_skewness</th>\n",
       "      <th>hsv_0_mean</th>\n",
       "      <th>hsv_0_std</th>\n",
       "      <th>hsv_0_skewness</th>\n",
       "      <th>hsv_1_mean</th>\n",
       "      <th>hsv_1_std</th>\n",
       "      <th>hsv_1_skewness</th>\n",
       "      <th>hsv_2_mean</th>\n",
       "      <th>hsv_2_std</th>\n",
       "      <th>hsv_2_skewness</th>\n",
       "      <th>lab_0_mean</th>\n",
       "      <th>lab_0_std</th>\n",
       "      <th>lab_0_skewness</th>\n",
       "      <th>lab_1_mean</th>\n",
       "      <th>lab_1_std</th>\n",
       "      <th>lab_1_skewness</th>\n",
       "      <th>lab_2_mean</th>\n",
       "      <th>lab_2_std</th>\n",
       "      <th>lab_2_skewness</th>\n",
       "      <th>rgb_0_hist_0</th>\n",
       "      <th>rgb_0_hist_1</th>\n",
       "      <th>rgb_0_hist_2</th>\n",
       "      <th>rgb_0_hist_3</th>\n",
       "      <th>rgb_0_hist_4</th>\n",
       "      <th>rgb_0_hist_5</th>\n",
       "      <th>rgb_0_hist_6</th>\n",
       "      <th>rgb_0_hist_7</th>\n",
       "      <th>rgb_0_hist_8</th>\n",
       "      <th>rgb_0_hist_9</th>\n",
       "      <th>rgb_0_hist_10</th>\n",
       "      <th>rgb_0_hist_11</th>\n",
       "      <th>rgb_0_hist_12</th>\n",
       "      <th>rgb_0_hist_13</th>\n",
       "      <th>rgb_0_hist_14</th>\n",
       "      <th>rgb_0_hist_15</th>\n",
       "      <th>rgb_0_hist_16</th>\n",
       "      <th>rgb_0_hist_17</th>\n",
       "      <th>rgb_0_hist_18</th>\n",
       "      <th>rgb_0_hist_19</th>\n",
       "      <th>rgb_0_hist_20</th>\n",
       "      <th>rgb_0_hist_21</th>\n",
       "      <th>rgb_0_hist_22</th>\n",
       "      <th>rgb_0_hist_23</th>\n",
       "      <th>rgb_0_hist_24</th>\n",
       "      <th>rgb_0_hist_25</th>\n",
       "      <th>rgb_0_hist_26</th>\n",
       "      <th>rgb_0_hist_27</th>\n",
       "      <th>rgb_0_hist_28</th>\n",
       "      <th>rgb_0_hist_29</th>\n",
       "      <th>rgb_0_hist_30</th>\n",
       "      <th>rgb_0_hist_31</th>\n",
       "      <th>rgb_1_hist_0</th>\n",
       "      <th>rgb_1_hist_1</th>\n",
       "      <th>rgb_1_hist_2</th>\n",
       "      <th>rgb_1_hist_3</th>\n",
       "      <th>rgb_1_hist_4</th>\n",
       "      <th>rgb_1_hist_5</th>\n",
       "      <th>rgb_1_hist_6</th>\n",
       "      <th>rgb_1_hist_7</th>\n",
       "      <th>rgb_1_hist_8</th>\n",
       "      <th>rgb_1_hist_9</th>\n",
       "      <th>rgb_1_hist_10</th>\n",
       "      <th>rgb_1_hist_11</th>\n",
       "      <th>rgb_1_hist_12</th>\n",
       "      <th>rgb_1_hist_13</th>\n",
       "      <th>rgb_1_hist_14</th>\n",
       "      <th>rgb_1_hist_15</th>\n",
       "      <th>rgb_1_hist_16</th>\n",
       "      <th>rgb_1_hist_17</th>\n",
       "      <th>rgb_1_hist_18</th>\n",
       "      <th>rgb_1_hist_19</th>\n",
       "      <th>rgb_1_hist_20</th>\n",
       "      <th>rgb_1_hist_21</th>\n",
       "      <th>rgb_1_hist_22</th>\n",
       "      <th>rgb_1_hist_23</th>\n",
       "      <th>rgb_1_hist_24</th>\n",
       "      <th>rgb_1_hist_25</th>\n",
       "      <th>rgb_1_hist_26</th>\n",
       "      <th>rgb_1_hist_27</th>\n",
       "      <th>rgb_1_hist_28</th>\n",
       "      <th>rgb_1_hist_29</th>\n",
       "      <th>rgb_1_hist_30</th>\n",
       "      <th>rgb_1_hist_31</th>\n",
       "      <th>rgb_2_hist_0</th>\n",
       "      <th>rgb_2_hist_1</th>\n",
       "      <th>rgb_2_hist_2</th>\n",
       "      <th>rgb_2_hist_3</th>\n",
       "      <th>rgb_2_hist_4</th>\n",
       "      <th>rgb_2_hist_5</th>\n",
       "      <th>rgb_2_hist_6</th>\n",
       "      <th>rgb_2_hist_7</th>\n",
       "      <th>rgb_2_hist_8</th>\n",
       "      <th>rgb_2_hist_9</th>\n",
       "      <th>rgb_2_hist_10</th>\n",
       "      <th>rgb_2_hist_11</th>\n",
       "      <th>rgb_2_hist_12</th>\n",
       "      <th>rgb_2_hist_13</th>\n",
       "      <th>rgb_2_hist_14</th>\n",
       "      <th>rgb_2_hist_15</th>\n",
       "      <th>rgb_2_hist_16</th>\n",
       "      <th>rgb_2_hist_17</th>\n",
       "      <th>rgb_2_hist_18</th>\n",
       "      <th>rgb_2_hist_19</th>\n",
       "      <th>rgb_2_hist_20</th>\n",
       "      <th>rgb_2_hist_21</th>\n",
       "      <th>rgb_2_hist_22</th>\n",
       "      <th>rgb_2_hist_23</th>\n",
       "      <th>rgb_2_hist_24</th>\n",
       "      <th>rgb_2_hist_25</th>\n",
       "      <th>rgb_2_hist_26</th>\n",
       "      <th>rgb_2_hist_27</th>\n",
       "      <th>rgb_2_hist_28</th>\n",
       "      <th>rgb_2_hist_29</th>\n",
       "      <th>rgb_2_hist_30</th>\n",
       "      <th>rgb_2_hist_31</th>\n",
       "      <th>hsv_0_hist_0</th>\n",
       "      <th>hsv_0_hist_1</th>\n",
       "      <th>hsv_0_hist_2</th>\n",
       "      <th>hsv_0_hist_3</th>\n",
       "      <th>hsv_0_hist_4</th>\n",
       "      <th>hsv_0_hist_5</th>\n",
       "      <th>hsv_0_hist_6</th>\n",
       "      <th>hsv_0_hist_7</th>\n",
       "      <th>hsv_0_hist_8</th>\n",
       "      <th>hsv_0_hist_9</th>\n",
       "      <th>hsv_0_hist_10</th>\n",
       "      <th>hsv_0_hist_11</th>\n",
       "      <th>hsv_0_hist_12</th>\n",
       "      <th>hsv_0_hist_13</th>\n",
       "      <th>hsv_0_hist_14</th>\n",
       "      <th>hsv_0_hist_15</th>\n",
       "      <th>hsv_0_hist_16</th>\n",
       "      <th>hsv_0_hist_17</th>\n",
       "      <th>hsv_0_hist_18</th>\n",
       "      <th>hsv_0_hist_19</th>\n",
       "      <th>hsv_0_hist_20</th>\n",
       "      <th>hsv_0_hist_21</th>\n",
       "      <th>hsv_0_hist_22</th>\n",
       "      <th>hsv_0_hist_23</th>\n",
       "      <th>hsv_0_hist_24</th>\n",
       "      <th>hsv_0_hist_25</th>\n",
       "      <th>hsv_0_hist_26</th>\n",
       "      <th>hsv_0_hist_27</th>\n",
       "      <th>hsv_0_hist_28</th>\n",
       "      <th>hsv_0_hist_29</th>\n",
       "      <th>hsv_0_hist_30</th>\n",
       "      <th>hsv_0_hist_31</th>\n",
       "      <th>hsv_1_hist_0</th>\n",
       "      <th>hsv_1_hist_1</th>\n",
       "      <th>hsv_1_hist_2</th>\n",
       "      <th>hsv_1_hist_3</th>\n",
       "      <th>hsv_1_hist_4</th>\n",
       "      <th>hsv_1_hist_5</th>\n",
       "      <th>hsv_1_hist_6</th>\n",
       "      <th>hsv_1_hist_7</th>\n",
       "      <th>hsv_1_hist_8</th>\n",
       "      <th>hsv_1_hist_9</th>\n",
       "      <th>hsv_1_hist_10</th>\n",
       "      <th>hsv_1_hist_11</th>\n",
       "      <th>hsv_1_hist_12</th>\n",
       "      <th>hsv_1_hist_13</th>\n",
       "      <th>hsv_1_hist_14</th>\n",
       "      <th>hsv_1_hist_15</th>\n",
       "      <th>hsv_1_hist_16</th>\n",
       "      <th>hsv_1_hist_17</th>\n",
       "      <th>hsv_1_hist_18</th>\n",
       "      <th>hsv_1_hist_19</th>\n",
       "      <th>hsv_1_hist_20</th>\n",
       "      <th>hsv_1_hist_21</th>\n",
       "      <th>hsv_1_hist_22</th>\n",
       "      <th>hsv_1_hist_23</th>\n",
       "      <th>hsv_1_hist_24</th>\n",
       "      <th>hsv_1_hist_25</th>\n",
       "      <th>hsv_1_hist_26</th>\n",
       "      <th>hsv_1_hist_27</th>\n",
       "      <th>hsv_1_hist_28</th>\n",
       "      <th>hsv_1_hist_29</th>\n",
       "      <th>hsv_1_hist_30</th>\n",
       "      <th>hsv_1_hist_31</th>\n",
       "      <th>hsv_2_hist_0</th>\n",
       "      <th>hsv_2_hist_1</th>\n",
       "      <th>hsv_2_hist_2</th>\n",
       "      <th>hsv_2_hist_3</th>\n",
       "      <th>hsv_2_hist_4</th>\n",
       "      <th>hsv_2_hist_5</th>\n",
       "      <th>hsv_2_hist_6</th>\n",
       "      <th>hsv_2_hist_7</th>\n",
       "      <th>hsv_2_hist_8</th>\n",
       "      <th>hsv_2_hist_9</th>\n",
       "      <th>hsv_2_hist_10</th>\n",
       "      <th>hsv_2_hist_11</th>\n",
       "      <th>hsv_2_hist_12</th>\n",
       "      <th>hsv_2_hist_13</th>\n",
       "      <th>hsv_2_hist_14</th>\n",
       "      <th>hsv_2_hist_15</th>\n",
       "      <th>hsv_2_hist_16</th>\n",
       "      <th>hsv_2_hist_17</th>\n",
       "      <th>hsv_2_hist_18</th>\n",
       "      <th>hsv_2_hist_19</th>\n",
       "      <th>hsv_2_hist_20</th>\n",
       "      <th>hsv_2_hist_21</th>\n",
       "      <th>hsv_2_hist_22</th>\n",
       "      <th>hsv_2_hist_23</th>\n",
       "      <th>hsv_2_hist_24</th>\n",
       "      <th>hsv_2_hist_25</th>\n",
       "      <th>hsv_2_hist_26</th>\n",
       "      <th>hsv_2_hist_27</th>\n",
       "      <th>hsv_2_hist_28</th>\n",
       "      <th>hsv_2_hist_29</th>\n",
       "      <th>hsv_2_hist_30</th>\n",
       "      <th>hsv_2_hist_31</th>\n",
       "      <th>lab_0_hist_0</th>\n",
       "      <th>lab_0_hist_1</th>\n",
       "      <th>lab_0_hist_2</th>\n",
       "      <th>lab_0_hist_3</th>\n",
       "      <th>lab_0_hist_4</th>\n",
       "      <th>lab_0_hist_5</th>\n",
       "      <th>lab_0_hist_6</th>\n",
       "      <th>lab_0_hist_7</th>\n",
       "      <th>lab_0_hist_8</th>\n",
       "      <th>lab_0_hist_9</th>\n",
       "      <th>lab_0_hist_10</th>\n",
       "      <th>lab_0_hist_11</th>\n",
       "      <th>lab_0_hist_12</th>\n",
       "      <th>lab_0_hist_13</th>\n",
       "      <th>lab_0_hist_14</th>\n",
       "      <th>lab_0_hist_15</th>\n",
       "      <th>lab_0_hist_16</th>\n",
       "      <th>lab_0_hist_17</th>\n",
       "      <th>lab_0_hist_18</th>\n",
       "      <th>lab_0_hist_19</th>\n",
       "      <th>lab_0_hist_20</th>\n",
       "      <th>lab_0_hist_21</th>\n",
       "      <th>lab_0_hist_22</th>\n",
       "      <th>lab_0_hist_23</th>\n",
       "      <th>lab_0_hist_24</th>\n",
       "      <th>lab_0_hist_25</th>\n",
       "      <th>lab_0_hist_26</th>\n",
       "      <th>lab_0_hist_27</th>\n",
       "      <th>lab_0_hist_28</th>\n",
       "      <th>lab_0_hist_29</th>\n",
       "      <th>lab_0_hist_30</th>\n",
       "      <th>lab_0_hist_31</th>\n",
       "      <th>lab_1_hist_0</th>\n",
       "      <th>lab_1_hist_1</th>\n",
       "      <th>lab_1_hist_2</th>\n",
       "      <th>lab_1_hist_3</th>\n",
       "      <th>lab_1_hist_4</th>\n",
       "      <th>lab_1_hist_5</th>\n",
       "      <th>lab_1_hist_6</th>\n",
       "      <th>lab_1_hist_7</th>\n",
       "      <th>lab_1_hist_8</th>\n",
       "      <th>lab_1_hist_9</th>\n",
       "      <th>lab_1_hist_10</th>\n",
       "      <th>lab_1_hist_11</th>\n",
       "      <th>lab_1_hist_12</th>\n",
       "      <th>lab_1_hist_13</th>\n",
       "      <th>lab_1_hist_14</th>\n",
       "      <th>lab_1_hist_15</th>\n",
       "      <th>lab_1_hist_16</th>\n",
       "      <th>lab_1_hist_17</th>\n",
       "      <th>lab_1_hist_18</th>\n",
       "      <th>lab_1_hist_19</th>\n",
       "      <th>lab_1_hist_20</th>\n",
       "      <th>lab_1_hist_21</th>\n",
       "      <th>lab_1_hist_22</th>\n",
       "      <th>lab_1_hist_23</th>\n",
       "      <th>lab_1_hist_24</th>\n",
       "      <th>lab_1_hist_25</th>\n",
       "      <th>lab_1_hist_26</th>\n",
       "      <th>lab_1_hist_27</th>\n",
       "      <th>lab_1_hist_28</th>\n",
       "      <th>lab_1_hist_29</th>\n",
       "      <th>lab_1_hist_30</th>\n",
       "      <th>lab_1_hist_31</th>\n",
       "      <th>lab_2_hist_0</th>\n",
       "      <th>lab_2_hist_1</th>\n",
       "      <th>lab_2_hist_2</th>\n",
       "      <th>lab_2_hist_3</th>\n",
       "      <th>lab_2_hist_4</th>\n",
       "      <th>lab_2_hist_5</th>\n",
       "      <th>lab_2_hist_6</th>\n",
       "      <th>lab_2_hist_7</th>\n",
       "      <th>lab_2_hist_8</th>\n",
       "      <th>lab_2_hist_9</th>\n",
       "      <th>lab_2_hist_10</th>\n",
       "      <th>lab_2_hist_11</th>\n",
       "      <th>lab_2_hist_12</th>\n",
       "      <th>lab_2_hist_13</th>\n",
       "      <th>lab_2_hist_14</th>\n",
       "      <th>lab_2_hist_15</th>\n",
       "      <th>lab_2_hist_16</th>\n",
       "      <th>lab_2_hist_17</th>\n",
       "      <th>lab_2_hist_18</th>\n",
       "      <th>lab_2_hist_19</th>\n",
       "      <th>lab_2_hist_20</th>\n",
       "      <th>lab_2_hist_21</th>\n",
       "      <th>lab_2_hist_22</th>\n",
       "      <th>lab_2_hist_23</th>\n",
       "      <th>lab_2_hist_24</th>\n",
       "      <th>lab_2_hist_25</th>\n",
       "      <th>lab_2_hist_26</th>\n",
       "      <th>lab_2_hist_27</th>\n",
       "      <th>lab_2_hist_28</th>\n",
       "      <th>lab_2_hist_29</th>\n",
       "      <th>lab_2_hist_30</th>\n",
       "      <th>lab_2_hist_31</th>\n",
       "      <th>dominant_color_0_ratio</th>\n",
       "      <th>dominant_color_1_ratio</th>\n",
       "      <th>dominant_color_2_ratio</th>\n",
       "      <th>dominant_color_3_ratio</th>\n",
       "      <th>dominant_color_4_ratio</th>\n",
       "      <th>green_ratio</th>\n",
       "      <th>saturation_ratio</th>\n",
       "      <th>value_ratio</th>\n",
       "      <th>glcm_contrast</th>\n",
       "      <th>glcm_dissimilarity</th>\n",
       "      <th>glcm_homogeneity</th>\n",
       "      <th>glcm_energy</th>\n",
       "      <th>glcm_correlation</th>\n",
       "      <th>lbp_hist_0</th>\n",
       "      <th>lbp_hist_1</th>\n",
       "      <th>lbp_hist_2</th>\n",
       "      <th>lbp_hist_3</th>\n",
       "      <th>lbp_hist_4</th>\n",
       "      <th>lbp_hist_5</th>\n",
       "      <th>lbp_hist_6</th>\n",
       "      <th>lbp_hist_7</th>\n",
       "      <th>lbp_hist_8</th>\n",
       "      <th>lbp_hist_9</th>\n",
       "      <th>lbp_hist_10</th>\n",
       "      <th>lbp_hist_11</th>\n",
       "      <th>lbp_hist_12</th>\n",
       "      <th>lbp_hist_13</th>\n",
       "      <th>lbp_hist_14</th>\n",
       "      <th>lbp_hist_15</th>\n",
       "      <th>lbp_hist_16</th>\n",
       "      <th>lbp_hist_17</th>\n",
       "      <th>lbp_hist_18</th>\n",
       "      <th>lbp_hist_19</th>\n",
       "      <th>lbp_hist_20</th>\n",
       "      <th>lbp_hist_21</th>\n",
       "      <th>lbp_hist_22</th>\n",
       "      <th>lbp_hist_23</th>\n",
       "      <th>lbp_hist_24</th>\n",
       "      <th>lbp_hist_25</th>\n",
       "      <th>edge_density</th>\n",
       "      <th>roughness_mean</th>\n",
       "      <th>roughness_std</th>\n",
       "      <th>area</th>\n",
       "      <th>perimeter</th>\n",
       "      <th>circularity</th>\n",
       "      <th>eccentricity</th>\n",
       "      <th>solidity</th>\n",
       "      <th>extent</th>\n",
       "      <th>vein_density</th>\n",
       "      <th>mean_orientation</th>\n",
       "      <th>orientation_std</th>\n",
       "      <th>branching_density</th>\n",
       "      <th>mean_segment_length</th>\n",
       "      <th>segment_length_std</th>\n",
       "      <th>vesselness_mean</th>\n",
       "      <th>vesselness_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Class1_19.webp</td>\n",
       "      <td>1Hibiscus rosa-sinensis(HRS)</td>\n",
       "      <td>1</td>\n",
       "      <td>188.705416</td>\n",
       "      <td>69.990575</td>\n",
       "      <td>-1.354648</td>\n",
       "      <td>200.608360</td>\n",
       "      <td>50.907016</td>\n",
       "      <td>-1.110815</td>\n",
       "      <td>196.463884</td>\n",
       "      <td>58.276169</td>\n",
       "      <td>-1.239846</td>\n",
       "      <td>63.958588</td>\n",
       "      <td>36.265557</td>\n",
       "      <td>-1.122727</td>\n",
       "      <td>27.474816</td>\n",
       "      <td>55.044592</td>\n",
       "      <td>1.993572</td>\n",
       "      <td>200.732288</td>\n",
       "      <td>50.759808</td>\n",
       "      <td>-1.110568</td>\n",
       "      <td>202.662404</td>\n",
       "      <td>51.991219</td>\n",
       "      <td>-1.237514</td>\n",
       "      <td>122.988884</td>\n",
       "      <td>9.028771</td>\n",
       "      <td>-1.966733</td>\n",
       "      <td>129.199640</td>\n",
       "      <td>3.215881</td>\n",
       "      <td>1.895660</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.001372</td>\n",
       "      <td>0.006260</td>\n",
       "      <td>0.026896</td>\n",
       "      <td>0.046820</td>\n",
       "      <td>0.041008</td>\n",
       "      <td>0.017992</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>0.004956</td>\n",
       "      <td>0.004536</td>\n",
       "      <td>0.004756</td>\n",
       "      <td>0.004272</td>\n",
       "      <td>0.003732</td>\n",
       "      <td>0.002320</td>\n",
       "      <td>0.001872</td>\n",
       "      <td>0.001232</td>\n",
       "      <td>0.000932</td>\n",
       "      <td>0.000912</td>\n",
       "      <td>0.000768</td>\n",
       "      <td>0.001128</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>0.001156</td>\n",
       "      <td>0.001904</td>\n",
       "      <td>0.003572</td>\n",
       "      <td>0.051484</td>\n",
       "      <td>0.353044</td>\n",
       "      <td>0.155904</td>\n",
       "      <td>0.001548</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.249936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.001252</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>0.001432</td>\n",
       "      <td>0.005632</td>\n",
       "      <td>0.014448</td>\n",
       "      <td>0.032056</td>\n",
       "      <td>0.044912</td>\n",
       "      <td>0.053304</td>\n",
       "      <td>0.013524</td>\n",
       "      <td>0.003932</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>0.001852</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>0.001068</td>\n",
       "      <td>0.001124</td>\n",
       "      <td>0.001268</td>\n",
       "      <td>0.001868</td>\n",
       "      <td>0.011324</td>\n",
       "      <td>0.282180</td>\n",
       "      <td>0.251576</td>\n",
       "      <td>0.021156</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.001152</td>\n",
       "      <td>0.004020</td>\n",
       "      <td>0.010832</td>\n",
       "      <td>0.033844</td>\n",
       "      <td>0.052064</td>\n",
       "      <td>0.041012</td>\n",
       "      <td>0.012548</td>\n",
       "      <td>0.006632</td>\n",
       "      <td>0.004388</td>\n",
       "      <td>0.003692</td>\n",
       "      <td>0.002264</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.001984</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>0.001084</td>\n",
       "      <td>0.001084</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>0.001388</td>\n",
       "      <td>0.002328</td>\n",
       "      <td>0.019492</td>\n",
       "      <td>0.305468</td>\n",
       "      <td>0.232788</td>\n",
       "      <td>0.007572</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.237140</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.001640</td>\n",
       "      <td>0.001376</td>\n",
       "      <td>0.009608</td>\n",
       "      <td>0.015672</td>\n",
       "      <td>0.154988</td>\n",
       "      <td>0.352220</td>\n",
       "      <td>0.197080</td>\n",
       "      <td>0.018640</td>\n",
       "      <td>0.008592</td>\n",
       "      <td>0.001568</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000544</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.791756</td>\n",
       "      <td>0.025052</td>\n",
       "      <td>0.004524</td>\n",
       "      <td>0.005696</td>\n",
       "      <td>0.008972</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.002740</td>\n",
       "      <td>0.001960</td>\n",
       "      <td>0.001812</td>\n",
       "      <td>0.001936</td>\n",
       "      <td>0.001620</td>\n",
       "      <td>0.002016</td>\n",
       "      <td>0.002644</td>\n",
       "      <td>0.003524</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>0.006768</td>\n",
       "      <td>0.009632</td>\n",
       "      <td>0.010944</td>\n",
       "      <td>0.013372</td>\n",
       "      <td>0.021424</td>\n",
       "      <td>0.026444</td>\n",
       "      <td>0.022620</td>\n",
       "      <td>0.013632</td>\n",
       "      <td>0.006224</td>\n",
       "      <td>0.003032</td>\n",
       "      <td>0.001568</td>\n",
       "      <td>0.000736</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>0.005460</td>\n",
       "      <td>0.014312</td>\n",
       "      <td>0.032092</td>\n",
       "      <td>0.044452</td>\n",
       "      <td>0.053112</td>\n",
       "      <td>0.013652</td>\n",
       "      <td>0.003840</td>\n",
       "      <td>0.002376</td>\n",
       "      <td>0.002716</td>\n",
       "      <td>0.001464</td>\n",
       "      <td>0.001256</td>\n",
       "      <td>0.001132</td>\n",
       "      <td>0.001060</td>\n",
       "      <td>0.001236</td>\n",
       "      <td>0.001948</td>\n",
       "      <td>0.010696</td>\n",
       "      <td>0.273740</td>\n",
       "      <td>0.260160</td>\n",
       "      <td>0.021760</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.001264</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.002360</td>\n",
       "      <td>0.007704</td>\n",
       "      <td>0.019512</td>\n",
       "      <td>0.036740</td>\n",
       "      <td>0.049760</td>\n",
       "      <td>0.039172</td>\n",
       "      <td>0.009180</td>\n",
       "      <td>0.003612</td>\n",
       "      <td>0.002928</td>\n",
       "      <td>0.002060</td>\n",
       "      <td>0.001384</td>\n",
       "      <td>0.001060</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.001064</td>\n",
       "      <td>0.001228</td>\n",
       "      <td>0.001572</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>0.040524</td>\n",
       "      <td>0.371596</td>\n",
       "      <td>0.151488</td>\n",
       "      <td>0.001648</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.113476</td>\n",
       "      <td>0.031416</td>\n",
       "      <td>0.013176</td>\n",
       "      <td>0.593596</td>\n",
       "      <td>0.248168</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.001148</td>\n",
       "      <td>0.043396</td>\n",
       "      <td>0.846508</td>\n",
       "      <td>0.108940</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.568708</td>\n",
       "      <td>0.143312</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.029140</td>\n",
       "      <td>0.008840</td>\n",
       "      <td>0.520832</td>\n",
       "      <td>0.107744</td>\n",
       "      <td>0.787185</td>\n",
       "      <td>390.718182</td>\n",
       "      <td>3.068500</td>\n",
       "      <td>0.855175</td>\n",
       "      <td>0.592378</td>\n",
       "      <td>0.977475</td>\n",
       "      <td>0.003560</td>\n",
       "      <td>0.002832</td>\n",
       "      <td>0.002148</td>\n",
       "      <td>0.002052</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>0.002108</td>\n",
       "      <td>0.002072</td>\n",
       "      <td>0.002944</td>\n",
       "      <td>0.003592</td>\n",
       "      <td>0.005068</td>\n",
       "      <td>0.006868</td>\n",
       "      <td>0.010460</td>\n",
       "      <td>0.014336</td>\n",
       "      <td>0.021124</td>\n",
       "      <td>0.008388</td>\n",
       "      <td>0.016320</td>\n",
       "      <td>0.005696</td>\n",
       "      <td>0.011304</td>\n",
       "      <td>0.004544</td>\n",
       "      <td>0.005596</td>\n",
       "      <td>0.003724</td>\n",
       "      <td>0.005488</td>\n",
       "      <td>0.003476</td>\n",
       "      <td>0.002592</td>\n",
       "      <td>0.781676</td>\n",
       "      <td>0.070192</td>\n",
       "      <td>0.012512</td>\n",
       "      <td>12.934366</td>\n",
       "      <td>77.796735</td>\n",
       "      <td>40741.0</td>\n",
       "      <td>1735.602155</td>\n",
       "      <td>0.169958</td>\n",
       "      <td>0.736233</td>\n",
       "      <td>0.914860</td>\n",
       "      <td>0.638294</td>\n",
       "      <td>0.009984</td>\n",
       "      <td>1.570723</td>\n",
       "      <td>1.028328</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>8.720785</td>\n",
       "      <td>0.010135</td>\n",
       "      <td>0.060742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Class1_93.webp</td>\n",
       "      <td>1Hibiscus rosa-sinensis(HRS)</td>\n",
       "      <td>1</td>\n",
       "      <td>149.297972</td>\n",
       "      <td>108.692582</td>\n",
       "      <td>-0.315699</td>\n",
       "      <td>181.381852</td>\n",
       "      <td>71.554596</td>\n",
       "      <td>-0.283444</td>\n",
       "      <td>162.624352</td>\n",
       "      <td>95.863944</td>\n",
       "      <td>-0.331065</td>\n",
       "      <td>67.937152</td>\n",
       "      <td>46.860072</td>\n",
       "      <td>-0.202374</td>\n",
       "      <td>85.817992</td>\n",
       "      <td>99.754710</td>\n",
       "      <td>0.489063</td>\n",
       "      <td>182.864468</td>\n",
       "      <td>72.367514</td>\n",
       "      <td>-0.305200</td>\n",
       "      <td>180.119244</td>\n",
       "      <td>74.785723</td>\n",
       "      <td>-0.301763</td>\n",
       "      <td>114.000272</td>\n",
       "      <td>17.040622</td>\n",
       "      <td>-0.363435</td>\n",
       "      <td>136.101280</td>\n",
       "      <td>11.361412</td>\n",
       "      <td>0.416876</td>\n",
       "      <td>0.072368</td>\n",
       "      <td>0.112692</td>\n",
       "      <td>0.087504</td>\n",
       "      <td>0.055740</td>\n",
       "      <td>0.024880</td>\n",
       "      <td>0.015100</td>\n",
       "      <td>0.010480</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>0.006988</td>\n",
       "      <td>0.005736</td>\n",
       "      <td>0.004588</td>\n",
       "      <td>0.004116</td>\n",
       "      <td>0.003956</td>\n",
       "      <td>0.004132</td>\n",
       "      <td>0.004020</td>\n",
       "      <td>0.004412</td>\n",
       "      <td>0.003628</td>\n",
       "      <td>0.004860</td>\n",
       "      <td>0.002940</td>\n",
       "      <td>0.001216</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>0.000852</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>0.001104</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>0.001532</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.009324</td>\n",
       "      <td>0.062896</td>\n",
       "      <td>0.179048</td>\n",
       "      <td>0.052552</td>\n",
       "      <td>0.250248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>0.007032</td>\n",
       "      <td>0.063328</td>\n",
       "      <td>0.130272</td>\n",
       "      <td>0.090976</td>\n",
       "      <td>0.056196</td>\n",
       "      <td>0.029280</td>\n",
       "      <td>0.015476</td>\n",
       "      <td>0.010436</td>\n",
       "      <td>0.009424</td>\n",
       "      <td>0.008280</td>\n",
       "      <td>0.009176</td>\n",
       "      <td>0.003012</td>\n",
       "      <td>0.001920</td>\n",
       "      <td>0.001388</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.001208</td>\n",
       "      <td>0.001244</td>\n",
       "      <td>0.001644</td>\n",
       "      <td>0.004576</td>\n",
       "      <td>0.107056</td>\n",
       "      <td>0.156856</td>\n",
       "      <td>0.039188</td>\n",
       "      <td>0.250072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.040384</td>\n",
       "      <td>0.096580</td>\n",
       "      <td>0.071772</td>\n",
       "      <td>0.069432</td>\n",
       "      <td>0.053628</td>\n",
       "      <td>0.029980</td>\n",
       "      <td>0.016832</td>\n",
       "      <td>0.010784</td>\n",
       "      <td>0.007536</td>\n",
       "      <td>0.006336</td>\n",
       "      <td>0.005104</td>\n",
       "      <td>0.003948</td>\n",
       "      <td>0.003432</td>\n",
       "      <td>0.003036</td>\n",
       "      <td>0.002464</td>\n",
       "      <td>0.002484</td>\n",
       "      <td>0.002548</td>\n",
       "      <td>0.005112</td>\n",
       "      <td>0.005208</td>\n",
       "      <td>0.001088</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>0.001240</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.003224</td>\n",
       "      <td>0.034200</td>\n",
       "      <td>0.158528</td>\n",
       "      <td>0.111376</td>\n",
       "      <td>0.250084</td>\n",
       "      <td>0.259416</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.005956</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.001548</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.004004</td>\n",
       "      <td>0.282264</td>\n",
       "      <td>0.134908</td>\n",
       "      <td>0.010520</td>\n",
       "      <td>0.018860</td>\n",
       "      <td>0.007380</td>\n",
       "      <td>0.050316</td>\n",
       "      <td>0.017316</td>\n",
       "      <td>0.042160</td>\n",
       "      <td>0.149356</td>\n",
       "      <td>0.002768</td>\n",
       "      <td>0.007136</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.002156</td>\n",
       "      <td>0.001040</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.477616</td>\n",
       "      <td>0.075800</td>\n",
       "      <td>0.004976</td>\n",
       "      <td>0.002496</td>\n",
       "      <td>0.004316</td>\n",
       "      <td>0.008676</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>0.003388</td>\n",
       "      <td>0.003620</td>\n",
       "      <td>0.004352</td>\n",
       "      <td>0.004604</td>\n",
       "      <td>0.003636</td>\n",
       "      <td>0.004416</td>\n",
       "      <td>0.004496</td>\n",
       "      <td>0.004976</td>\n",
       "      <td>0.005596</td>\n",
       "      <td>0.005944</td>\n",
       "      <td>0.006328</td>\n",
       "      <td>0.007352</td>\n",
       "      <td>0.007836</td>\n",
       "      <td>0.010536</td>\n",
       "      <td>0.012560</td>\n",
       "      <td>0.018940</td>\n",
       "      <td>0.026104</td>\n",
       "      <td>0.036088</td>\n",
       "      <td>0.036528</td>\n",
       "      <td>0.041420</td>\n",
       "      <td>0.042212</td>\n",
       "      <td>0.045968</td>\n",
       "      <td>0.036836</td>\n",
       "      <td>0.029892</td>\n",
       "      <td>0.018584</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>0.007032</td>\n",
       "      <td>0.063316</td>\n",
       "      <td>0.130180</td>\n",
       "      <td>0.090936</td>\n",
       "      <td>0.056232</td>\n",
       "      <td>0.028784</td>\n",
       "      <td>0.014652</td>\n",
       "      <td>0.009968</td>\n",
       "      <td>0.007388</td>\n",
       "      <td>0.006532</td>\n",
       "      <td>0.005572</td>\n",
       "      <td>0.007104</td>\n",
       "      <td>0.006260</td>\n",
       "      <td>0.001624</td>\n",
       "      <td>0.001124</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>0.001336</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.002668</td>\n",
       "      <td>0.034228</td>\n",
       "      <td>0.157240</td>\n",
       "      <td>0.114012</td>\n",
       "      <td>0.250352</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.003436</td>\n",
       "      <td>0.039404</td>\n",
       "      <td>0.124684</td>\n",
       "      <td>0.101328</td>\n",
       "      <td>0.065192</td>\n",
       "      <td>0.034756</td>\n",
       "      <td>0.016704</td>\n",
       "      <td>0.011012</td>\n",
       "      <td>0.008412</td>\n",
       "      <td>0.007284</td>\n",
       "      <td>0.007376</td>\n",
       "      <td>0.006992</td>\n",
       "      <td>0.006988</td>\n",
       "      <td>0.001944</td>\n",
       "      <td>0.001228</td>\n",
       "      <td>0.001064</td>\n",
       "      <td>0.001132</td>\n",
       "      <td>0.001268</td>\n",
       "      <td>0.001460</td>\n",
       "      <td>0.002652</td>\n",
       "      <td>0.041552</td>\n",
       "      <td>0.135612</td>\n",
       "      <td>0.128212</td>\n",
       "      <td>0.250072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.321072</td>\n",
       "      <td>0.081688</td>\n",
       "      <td>0.015808</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.045904</td>\n",
       "      <td>0.529332</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011848</td>\n",
       "      <td>0.271148</td>\n",
       "      <td>0.303512</td>\n",
       "      <td>0.028516</td>\n",
       "      <td>0.251820</td>\n",
       "      <td>0.133156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.314116</td>\n",
       "      <td>0.310596</td>\n",
       "      <td>0.090152</td>\n",
       "      <td>0.250296</td>\n",
       "      <td>0.034840</td>\n",
       "      <td>0.581497</td>\n",
       "      <td>0.336541</td>\n",
       "      <td>0.717116</td>\n",
       "      <td>582.706028</td>\n",
       "      <td>5.611343</td>\n",
       "      <td>0.652550</td>\n",
       "      <td>0.338181</td>\n",
       "      <td>0.956031</td>\n",
       "      <td>0.012568</td>\n",
       "      <td>0.007900</td>\n",
       "      <td>0.006644</td>\n",
       "      <td>0.005724</td>\n",
       "      <td>0.005732</td>\n",
       "      <td>0.005584</td>\n",
       "      <td>0.005540</td>\n",
       "      <td>0.008140</td>\n",
       "      <td>0.009216</td>\n",
       "      <td>0.012144</td>\n",
       "      <td>0.016048</td>\n",
       "      <td>0.022756</td>\n",
       "      <td>0.030440</td>\n",
       "      <td>0.041632</td>\n",
       "      <td>0.017488</td>\n",
       "      <td>0.030060</td>\n",
       "      <td>0.012420</td>\n",
       "      <td>0.022140</td>\n",
       "      <td>0.011180</td>\n",
       "      <td>0.012556</td>\n",
       "      <td>0.010404</td>\n",
       "      <td>0.012468</td>\n",
       "      <td>0.010268</td>\n",
       "      <td>0.008288</td>\n",
       "      <td>0.490324</td>\n",
       "      <td>0.172336</td>\n",
       "      <td>0.035880</td>\n",
       "      <td>24.287410</td>\n",
       "      <td>92.470516</td>\n",
       "      <td>110328.5</td>\n",
       "      <td>2543.148399</td>\n",
       "      <td>0.214365</td>\n",
       "      <td>0.575504</td>\n",
       "      <td>0.940362</td>\n",
       "      <td>0.674998</td>\n",
       "      <td>0.007540</td>\n",
       "      <td>1.559771</td>\n",
       "      <td>1.028328</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>2.429124</td>\n",
       "      <td>2.528218</td>\n",
       "      <td>0.006045</td>\n",
       "      <td>0.039673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Class1_3.webp</td>\n",
       "      <td>1Hibiscus rosa-sinensis(HRS)</td>\n",
       "      <td>1</td>\n",
       "      <td>156.211568</td>\n",
       "      <td>103.263263</td>\n",
       "      <td>-0.424134</td>\n",
       "      <td>186.153564</td>\n",
       "      <td>66.936745</td>\n",
       "      <td>-0.355221</td>\n",
       "      <td>173.055608</td>\n",
       "      <td>84.571048</td>\n",
       "      <td>-0.401460</td>\n",
       "      <td>72.805396</td>\n",
       "      <td>48.344912</td>\n",
       "      <td>-0.277568</td>\n",
       "      <td>75.240344</td>\n",
       "      <td>93.881327</td>\n",
       "      <td>0.737977</td>\n",
       "      <td>187.413092</td>\n",
       "      <td>67.553960</td>\n",
       "      <td>-0.373898</td>\n",
       "      <td>185.460908</td>\n",
       "      <td>69.723658</td>\n",
       "      <td>-0.380676</td>\n",
       "      <td>115.813816</td>\n",
       "      <td>15.422224</td>\n",
       "      <td>-0.494102</td>\n",
       "      <td>132.942916</td>\n",
       "      <td>7.584343</td>\n",
       "      <td>0.511862</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.091884</td>\n",
       "      <td>0.049820</td>\n",
       "      <td>0.030564</td>\n",
       "      <td>0.025980</td>\n",
       "      <td>0.024468</td>\n",
       "      <td>0.020172</td>\n",
       "      <td>0.016252</td>\n",
       "      <td>0.014020</td>\n",
       "      <td>0.011656</td>\n",
       "      <td>0.009728</td>\n",
       "      <td>0.008448</td>\n",
       "      <td>0.006960</td>\n",
       "      <td>0.008636</td>\n",
       "      <td>0.011040</td>\n",
       "      <td>0.010324</td>\n",
       "      <td>0.009260</td>\n",
       "      <td>0.005492</td>\n",
       "      <td>0.002712</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.001032</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>0.001104</td>\n",
       "      <td>0.001484</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.022636</td>\n",
       "      <td>0.090500</td>\n",
       "      <td>0.170484</td>\n",
       "      <td>0.031800</td>\n",
       "      <td>0.250072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000424</td>\n",
       "      <td>0.020828</td>\n",
       "      <td>0.085036</td>\n",
       "      <td>0.091808</td>\n",
       "      <td>0.078008</td>\n",
       "      <td>0.043656</td>\n",
       "      <td>0.024228</td>\n",
       "      <td>0.021992</td>\n",
       "      <td>0.018128</td>\n",
       "      <td>0.012452</td>\n",
       "      <td>0.008108</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>0.006724</td>\n",
       "      <td>0.003988</td>\n",
       "      <td>0.002092</td>\n",
       "      <td>0.001248</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>0.001588</td>\n",
       "      <td>0.005748</td>\n",
       "      <td>0.128988</td>\n",
       "      <td>0.182884</td>\n",
       "      <td>0.001968</td>\n",
       "      <td>0.250012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.019236</td>\n",
       "      <td>0.083580</td>\n",
       "      <td>0.064548</td>\n",
       "      <td>0.068648</td>\n",
       "      <td>0.047260</td>\n",
       "      <td>0.031932</td>\n",
       "      <td>0.021608</td>\n",
       "      <td>0.014948</td>\n",
       "      <td>0.012044</td>\n",
       "      <td>0.008684</td>\n",
       "      <td>0.007652</td>\n",
       "      <td>0.008776</td>\n",
       "      <td>0.012412</td>\n",
       "      <td>0.011656</td>\n",
       "      <td>0.007552</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>0.001236</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>0.000996</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>0.001156</td>\n",
       "      <td>0.001640</td>\n",
       "      <td>0.004376</td>\n",
       "      <td>0.068936</td>\n",
       "      <td>0.185504</td>\n",
       "      <td>0.060108</td>\n",
       "      <td>0.250004</td>\n",
       "      <td>0.246892</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.004668</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.001464</td>\n",
       "      <td>0.030120</td>\n",
       "      <td>0.366952</td>\n",
       "      <td>0.020760</td>\n",
       "      <td>0.014824</td>\n",
       "      <td>0.023364</td>\n",
       "      <td>0.052192</td>\n",
       "      <td>0.015388</td>\n",
       "      <td>0.055212</td>\n",
       "      <td>0.089752</td>\n",
       "      <td>0.010220</td>\n",
       "      <td>0.048488</td>\n",
       "      <td>0.011976</td>\n",
       "      <td>0.004488</td>\n",
       "      <td>0.001512</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.533820</td>\n",
       "      <td>0.033408</td>\n",
       "      <td>0.003636</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.006416</td>\n",
       "      <td>0.007780</td>\n",
       "      <td>0.012776</td>\n",
       "      <td>0.009956</td>\n",
       "      <td>0.007648</td>\n",
       "      <td>0.007236</td>\n",
       "      <td>0.008396</td>\n",
       "      <td>0.009372</td>\n",
       "      <td>0.009732</td>\n",
       "      <td>0.010556</td>\n",
       "      <td>0.010740</td>\n",
       "      <td>0.011952</td>\n",
       "      <td>0.011368</td>\n",
       "      <td>0.012724</td>\n",
       "      <td>0.013092</td>\n",
       "      <td>0.013008</td>\n",
       "      <td>0.011140</td>\n",
       "      <td>0.011312</td>\n",
       "      <td>0.011540</td>\n",
       "      <td>0.011440</td>\n",
       "      <td>0.013652</td>\n",
       "      <td>0.017840</td>\n",
       "      <td>0.025464</td>\n",
       "      <td>0.033920</td>\n",
       "      <td>0.039196</td>\n",
       "      <td>0.046096</td>\n",
       "      <td>0.030152</td>\n",
       "      <td>0.011932</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>0.020788</td>\n",
       "      <td>0.085040</td>\n",
       "      <td>0.091848</td>\n",
       "      <td>0.077924</td>\n",
       "      <td>0.043600</td>\n",
       "      <td>0.023944</td>\n",
       "      <td>0.016056</td>\n",
       "      <td>0.016404</td>\n",
       "      <td>0.014572</td>\n",
       "      <td>0.011640</td>\n",
       "      <td>0.010816</td>\n",
       "      <td>0.006804</td>\n",
       "      <td>0.004072</td>\n",
       "      <td>0.002112</td>\n",
       "      <td>0.001216</td>\n",
       "      <td>0.001224</td>\n",
       "      <td>0.001292</td>\n",
       "      <td>0.003416</td>\n",
       "      <td>0.068564</td>\n",
       "      <td>0.180272</td>\n",
       "      <td>0.067868</td>\n",
       "      <td>0.250116</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.006692</td>\n",
       "      <td>0.071212</td>\n",
       "      <td>0.089240</td>\n",
       "      <td>0.081572</td>\n",
       "      <td>0.052668</td>\n",
       "      <td>0.029288</td>\n",
       "      <td>0.016796</td>\n",
       "      <td>0.016764</td>\n",
       "      <td>0.016932</td>\n",
       "      <td>0.013220</td>\n",
       "      <td>0.010564</td>\n",
       "      <td>0.009156</td>\n",
       "      <td>0.006620</td>\n",
       "      <td>0.003772</td>\n",
       "      <td>0.001720</td>\n",
       "      <td>0.001172</td>\n",
       "      <td>0.001232</td>\n",
       "      <td>0.001360</td>\n",
       "      <td>0.002732</td>\n",
       "      <td>0.064084</td>\n",
       "      <td>0.196512</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.250008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188476</td>\n",
       "      <td>0.153000</td>\n",
       "      <td>0.058592</td>\n",
       "      <td>0.009068</td>\n",
       "      <td>0.061776</td>\n",
       "      <td>0.529088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004680</td>\n",
       "      <td>0.309604</td>\n",
       "      <td>0.295860</td>\n",
       "      <td>0.247032</td>\n",
       "      <td>0.142824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.107552</td>\n",
       "      <td>0.250028</td>\n",
       "      <td>0.064836</td>\n",
       "      <td>0.322872</td>\n",
       "      <td>0.254712</td>\n",
       "      <td>0.565357</td>\n",
       "      <td>0.295060</td>\n",
       "      <td>0.734953</td>\n",
       "      <td>554.150002</td>\n",
       "      <td>5.658388</td>\n",
       "      <td>0.647332</td>\n",
       "      <td>0.302098</td>\n",
       "      <td>0.961747</td>\n",
       "      <td>0.013300</td>\n",
       "      <td>0.006572</td>\n",
       "      <td>0.006348</td>\n",
       "      <td>0.006048</td>\n",
       "      <td>0.005696</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.005652</td>\n",
       "      <td>0.007840</td>\n",
       "      <td>0.009036</td>\n",
       "      <td>0.011928</td>\n",
       "      <td>0.016192</td>\n",
       "      <td>0.023004</td>\n",
       "      <td>0.030160</td>\n",
       "      <td>0.050296</td>\n",
       "      <td>0.018328</td>\n",
       "      <td>0.029028</td>\n",
       "      <td>0.012716</td>\n",
       "      <td>0.026176</td>\n",
       "      <td>0.009832</td>\n",
       "      <td>0.010868</td>\n",
       "      <td>0.009296</td>\n",
       "      <td>0.010044</td>\n",
       "      <td>0.009140</td>\n",
       "      <td>0.006856</td>\n",
       "      <td>0.486776</td>\n",
       "      <td>0.173268</td>\n",
       "      <td>0.039868</td>\n",
       "      <td>24.471842</td>\n",
       "      <td>89.425351</td>\n",
       "      <td>106122.0</td>\n",
       "      <td>2715.793061</td>\n",
       "      <td>0.180810</td>\n",
       "      <td>0.576994</td>\n",
       "      <td>0.928529</td>\n",
       "      <td>0.625008</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>1.574884</td>\n",
       "      <td>1.028328</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>2.403846</td>\n",
       "      <td>2.498603</td>\n",
       "      <td>0.006453</td>\n",
       "      <td>0.038890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Class1_37.webp</td>\n",
       "      <td>1Hibiscus rosa-sinensis(HRS)</td>\n",
       "      <td>1</td>\n",
       "      <td>163.233416</td>\n",
       "      <td>94.049086</td>\n",
       "      <td>-0.711829</td>\n",
       "      <td>189.664052</td>\n",
       "      <td>59.073714</td>\n",
       "      <td>-0.575906</td>\n",
       "      <td>177.131960</td>\n",
       "      <td>76.471072</td>\n",
       "      <td>-0.680517</td>\n",
       "      <td>60.176716</td>\n",
       "      <td>36.350931</td>\n",
       "      <td>-0.938488</td>\n",
       "      <td>61.858284</td>\n",
       "      <td>90.301038</td>\n",
       "      <td>0.955800</td>\n",
       "      <td>189.780572</td>\n",
       "      <td>59.030649</td>\n",
       "      <td>-0.579672</td>\n",
       "      <td>189.826872</td>\n",
       "      <td>61.983841</td>\n",
       "      <td>-0.642626</td>\n",
       "      <td>117.096448</td>\n",
       "      <td>15.298257</td>\n",
       "      <td>-0.907872</td>\n",
       "      <td>132.833328</td>\n",
       "      <td>7.749605</td>\n",
       "      <td>1.023750</td>\n",
       "      <td>0.019168</td>\n",
       "      <td>0.062048</td>\n",
       "      <td>0.085756</td>\n",
       "      <td>0.063616</td>\n",
       "      <td>0.028804</td>\n",
       "      <td>0.014560</td>\n",
       "      <td>0.009788</td>\n",
       "      <td>0.007616</td>\n",
       "      <td>0.004916</td>\n",
       "      <td>0.003936</td>\n",
       "      <td>0.003288</td>\n",
       "      <td>0.001876</td>\n",
       "      <td>0.001532</td>\n",
       "      <td>0.001268</td>\n",
       "      <td>0.002432</td>\n",
       "      <td>0.001576</td>\n",
       "      <td>0.000884</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>0.000880</td>\n",
       "      <td>0.001088</td>\n",
       "      <td>0.001408</td>\n",
       "      <td>0.002236</td>\n",
       "      <td>0.005404</td>\n",
       "      <td>0.181904</td>\n",
       "      <td>0.232596</td>\n",
       "      <td>0.009052</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.004376</td>\n",
       "      <td>0.023948</td>\n",
       "      <td>0.041152</td>\n",
       "      <td>0.053784</td>\n",
       "      <td>0.074480</td>\n",
       "      <td>0.067128</td>\n",
       "      <td>0.025040</td>\n",
       "      <td>0.009140</td>\n",
       "      <td>0.004576</td>\n",
       "      <td>0.003472</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.001896</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.000964</td>\n",
       "      <td>0.001172</td>\n",
       "      <td>0.001604</td>\n",
       "      <td>0.035580</td>\n",
       "      <td>0.309460</td>\n",
       "      <td>0.086812</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.007796</td>\n",
       "      <td>0.023912</td>\n",
       "      <td>0.041636</td>\n",
       "      <td>0.062716</td>\n",
       "      <td>0.070116</td>\n",
       "      <td>0.051140</td>\n",
       "      <td>0.021180</td>\n",
       "      <td>0.009976</td>\n",
       "      <td>0.006548</td>\n",
       "      <td>0.004872</td>\n",
       "      <td>0.002928</td>\n",
       "      <td>0.002884</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>0.002672</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>0.000912</td>\n",
       "      <td>0.000848</td>\n",
       "      <td>0.001032</td>\n",
       "      <td>0.001004</td>\n",
       "      <td>0.001472</td>\n",
       "      <td>0.003272</td>\n",
       "      <td>0.057152</td>\n",
       "      <td>0.342952</td>\n",
       "      <td>0.028928</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.258388</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.001864</td>\n",
       "      <td>0.001496</td>\n",
       "      <td>0.004880</td>\n",
       "      <td>0.015340</td>\n",
       "      <td>0.304580</td>\n",
       "      <td>0.223872</td>\n",
       "      <td>0.160100</td>\n",
       "      <td>0.014940</td>\n",
       "      <td>0.009124</td>\n",
       "      <td>0.002156</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.001280</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.660348</td>\n",
       "      <td>0.017024</td>\n",
       "      <td>0.004024</td>\n",
       "      <td>0.003252</td>\n",
       "      <td>0.003276</td>\n",
       "      <td>0.004044</td>\n",
       "      <td>0.003408</td>\n",
       "      <td>0.002228</td>\n",
       "      <td>0.001356</td>\n",
       "      <td>0.001384</td>\n",
       "      <td>0.001424</td>\n",
       "      <td>0.001416</td>\n",
       "      <td>0.001640</td>\n",
       "      <td>0.001832</td>\n",
       "      <td>0.002244</td>\n",
       "      <td>0.002768</td>\n",
       "      <td>0.004324</td>\n",
       "      <td>0.005836</td>\n",
       "      <td>0.007068</td>\n",
       "      <td>0.006832</td>\n",
       "      <td>0.008316</td>\n",
       "      <td>0.011336</td>\n",
       "      <td>0.015628</td>\n",
       "      <td>0.026448</td>\n",
       "      <td>0.035076</td>\n",
       "      <td>0.043076</td>\n",
       "      <td>0.042344</td>\n",
       "      <td>0.033308</td>\n",
       "      <td>0.021424</td>\n",
       "      <td>0.018012</td>\n",
       "      <td>0.007080</td>\n",
       "      <td>0.002224</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.004376</td>\n",
       "      <td>0.023900</td>\n",
       "      <td>0.041116</td>\n",
       "      <td>0.053572</td>\n",
       "      <td>0.074560</td>\n",
       "      <td>0.066880</td>\n",
       "      <td>0.023336</td>\n",
       "      <td>0.008664</td>\n",
       "      <td>0.006076</td>\n",
       "      <td>0.004328</td>\n",
       "      <td>0.003356</td>\n",
       "      <td>0.001916</td>\n",
       "      <td>0.001132</td>\n",
       "      <td>0.000976</td>\n",
       "      <td>0.001184</td>\n",
       "      <td>0.001512</td>\n",
       "      <td>0.032452</td>\n",
       "      <td>0.311164</td>\n",
       "      <td>0.088484</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.002448</td>\n",
       "      <td>0.011332</td>\n",
       "      <td>0.040244</td>\n",
       "      <td>0.048532</td>\n",
       "      <td>0.071448</td>\n",
       "      <td>0.076992</td>\n",
       "      <td>0.031552</td>\n",
       "      <td>0.011832</td>\n",
       "      <td>0.006924</td>\n",
       "      <td>0.004872</td>\n",
       "      <td>0.003568</td>\n",
       "      <td>0.002144</td>\n",
       "      <td>0.001048</td>\n",
       "      <td>0.000964</td>\n",
       "      <td>0.000916</td>\n",
       "      <td>0.001172</td>\n",
       "      <td>0.001452</td>\n",
       "      <td>0.003240</td>\n",
       "      <td>0.214140</td>\n",
       "      <td>0.214796</td>\n",
       "      <td>0.000304</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.236076</td>\n",
       "      <td>0.048932</td>\n",
       "      <td>0.013752</td>\n",
       "      <td>0.009636</td>\n",
       "      <td>0.419376</td>\n",
       "      <td>0.272100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001676</td>\n",
       "      <td>0.034356</td>\n",
       "      <td>0.676472</td>\n",
       "      <td>0.084720</td>\n",
       "      <td>0.202016</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.146292</td>\n",
       "      <td>0.435780</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.144488</td>\n",
       "      <td>0.023440</td>\n",
       "      <td>0.557237</td>\n",
       "      <td>0.242582</td>\n",
       "      <td>0.744238</td>\n",
       "      <td>449.586281</td>\n",
       "      <td>4.396186</td>\n",
       "      <td>0.713631</td>\n",
       "      <td>0.388460</td>\n",
       "      <td>0.969847</td>\n",
       "      <td>0.008004</td>\n",
       "      <td>0.004972</td>\n",
       "      <td>0.004584</td>\n",
       "      <td>0.004404</td>\n",
       "      <td>0.004064</td>\n",
       "      <td>0.004180</td>\n",
       "      <td>0.004080</td>\n",
       "      <td>0.006056</td>\n",
       "      <td>0.007820</td>\n",
       "      <td>0.010220</td>\n",
       "      <td>0.013612</td>\n",
       "      <td>0.022296</td>\n",
       "      <td>0.032780</td>\n",
       "      <td>0.046636</td>\n",
       "      <td>0.015808</td>\n",
       "      <td>0.022632</td>\n",
       "      <td>0.009900</td>\n",
       "      <td>0.022732</td>\n",
       "      <td>0.007136</td>\n",
       "      <td>0.010144</td>\n",
       "      <td>0.006820</td>\n",
       "      <td>0.008192</td>\n",
       "      <td>0.006892</td>\n",
       "      <td>0.004988</td>\n",
       "      <td>0.568852</td>\n",
       "      <td>0.142196</td>\n",
       "      <td>0.018328</td>\n",
       "      <td>18.605150</td>\n",
       "      <td>80.847696</td>\n",
       "      <td>82218.0</td>\n",
       "      <td>2747.592050</td>\n",
       "      <td>0.136859</td>\n",
       "      <td>0.648862</td>\n",
       "      <td>0.867059</td>\n",
       "      <td>0.564130</td>\n",
       "      <td>0.012176</td>\n",
       "      <td>1.579425</td>\n",
       "      <td>1.028328</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>4.147139</td>\n",
       "      <td>10.222332</td>\n",
       "      <td>0.010463</td>\n",
       "      <td>0.060133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Class1_80.webp</td>\n",
       "      <td>1Hibiscus rosa-sinensis(HRS)</td>\n",
       "      <td>1</td>\n",
       "      <td>163.510904</td>\n",
       "      <td>90.259988</td>\n",
       "      <td>-0.751666</td>\n",
       "      <td>187.378692</td>\n",
       "      <td>58.149150</td>\n",
       "      <td>-0.605425</td>\n",
       "      <td>175.759856</td>\n",
       "      <td>73.987816</td>\n",
       "      <td>-0.703206</td>\n",
       "      <td>56.812688</td>\n",
       "      <td>35.514640</td>\n",
       "      <td>-0.738473</td>\n",
       "      <td>57.288048</td>\n",
       "      <td>85.206538</td>\n",
       "      <td>1.021426</td>\n",
       "      <td>187.485608</td>\n",
       "      <td>58.089104</td>\n",
       "      <td>-0.606174</td>\n",
       "      <td>188.002652</td>\n",
       "      <td>60.970680</td>\n",
       "      <td>-0.674752</td>\n",
       "      <td>117.896696</td>\n",
       "      <td>14.371689</td>\n",
       "      <td>-0.977628</td>\n",
       "      <td>132.454396</td>\n",
       "      <td>7.107394</td>\n",
       "      <td>1.096428</td>\n",
       "      <td>0.005116</td>\n",
       "      <td>0.042412</td>\n",
       "      <td>0.091056</td>\n",
       "      <td>0.065188</td>\n",
       "      <td>0.032408</td>\n",
       "      <td>0.023008</td>\n",
       "      <td>0.013552</td>\n",
       "      <td>0.008960</td>\n",
       "      <td>0.005112</td>\n",
       "      <td>0.003032</td>\n",
       "      <td>0.004048</td>\n",
       "      <td>0.003640</td>\n",
       "      <td>0.002832</td>\n",
       "      <td>0.001748</td>\n",
       "      <td>0.001184</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>0.000752</td>\n",
       "      <td>0.000832</td>\n",
       "      <td>0.000844</td>\n",
       "      <td>0.001032</td>\n",
       "      <td>0.001184</td>\n",
       "      <td>0.001436</td>\n",
       "      <td>0.001880</td>\n",
       "      <td>0.002776</td>\n",
       "      <td>0.010252</td>\n",
       "      <td>0.177144</td>\n",
       "      <td>0.290696</td>\n",
       "      <td>0.004696</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.202000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>0.001804</td>\n",
       "      <td>0.002912</td>\n",
       "      <td>0.022892</td>\n",
       "      <td>0.041884</td>\n",
       "      <td>0.081932</td>\n",
       "      <td>0.077772</td>\n",
       "      <td>0.044964</td>\n",
       "      <td>0.015544</td>\n",
       "      <td>0.006144</td>\n",
       "      <td>0.003492</td>\n",
       "      <td>0.001996</td>\n",
       "      <td>0.001668</td>\n",
       "      <td>0.001056</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>0.001356</td>\n",
       "      <td>0.002016</td>\n",
       "      <td>0.004480</td>\n",
       "      <td>0.071408</td>\n",
       "      <td>0.306208</td>\n",
       "      <td>0.104812</td>\n",
       "      <td>0.001088</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.202000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>0.002920</td>\n",
       "      <td>0.017088</td>\n",
       "      <td>0.038312</td>\n",
       "      <td>0.069784</td>\n",
       "      <td>0.081712</td>\n",
       "      <td>0.041920</td>\n",
       "      <td>0.017820</td>\n",
       "      <td>0.010928</td>\n",
       "      <td>0.008488</td>\n",
       "      <td>0.005668</td>\n",
       "      <td>0.004008</td>\n",
       "      <td>0.002536</td>\n",
       "      <td>0.001264</td>\n",
       "      <td>0.000924</td>\n",
       "      <td>0.000848</td>\n",
       "      <td>0.000956</td>\n",
       "      <td>0.001024</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.002524</td>\n",
       "      <td>0.007224</td>\n",
       "      <td>0.108740</td>\n",
       "      <td>0.304952</td>\n",
       "      <td>0.063476</td>\n",
       "      <td>0.000928</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.201996</td>\n",
       "      <td>0.253584</td>\n",
       "      <td>0.001888</td>\n",
       "      <td>0.001640</td>\n",
       "      <td>0.007656</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.048972</td>\n",
       "      <td>0.003116</td>\n",
       "      <td>0.031012</td>\n",
       "      <td>0.035520</td>\n",
       "      <td>0.300548</td>\n",
       "      <td>0.189468</td>\n",
       "      <td>0.100084</td>\n",
       "      <td>0.010480</td>\n",
       "      <td>0.011356</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.001120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.001108</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.662276</td>\n",
       "      <td>0.025048</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.003652</td>\n",
       "      <td>0.005744</td>\n",
       "      <td>0.003632</td>\n",
       "      <td>0.001596</td>\n",
       "      <td>0.001420</td>\n",
       "      <td>0.001196</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.002088</td>\n",
       "      <td>0.002888</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.005152</td>\n",
       "      <td>0.005592</td>\n",
       "      <td>0.007808</td>\n",
       "      <td>0.009068</td>\n",
       "      <td>0.012236</td>\n",
       "      <td>0.012496</td>\n",
       "      <td>0.014988</td>\n",
       "      <td>0.019088</td>\n",
       "      <td>0.027644</td>\n",
       "      <td>0.035540</td>\n",
       "      <td>0.046980</td>\n",
       "      <td>0.038008</td>\n",
       "      <td>0.024772</td>\n",
       "      <td>0.010588</td>\n",
       "      <td>0.004744</td>\n",
       "      <td>0.001768</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000940</td>\n",
       "      <td>0.001604</td>\n",
       "      <td>0.002576</td>\n",
       "      <td>0.022864</td>\n",
       "      <td>0.042004</td>\n",
       "      <td>0.081280</td>\n",
       "      <td>0.077624</td>\n",
       "      <td>0.045492</td>\n",
       "      <td>0.016060</td>\n",
       "      <td>0.006360</td>\n",
       "      <td>0.003548</td>\n",
       "      <td>0.001980</td>\n",
       "      <td>0.001672</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>0.001124</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.002012</td>\n",
       "      <td>0.004264</td>\n",
       "      <td>0.069828</td>\n",
       "      <td>0.307020</td>\n",
       "      <td>0.105844</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.202000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.001652</td>\n",
       "      <td>0.002060</td>\n",
       "      <td>0.010264</td>\n",
       "      <td>0.031560</td>\n",
       "      <td>0.073020</td>\n",
       "      <td>0.083828</td>\n",
       "      <td>0.056660</td>\n",
       "      <td>0.025412</td>\n",
       "      <td>0.009212</td>\n",
       "      <td>0.004784</td>\n",
       "      <td>0.002468</td>\n",
       "      <td>0.001568</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.000896</td>\n",
       "      <td>0.001068</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>0.001792</td>\n",
       "      <td>0.002704</td>\n",
       "      <td>0.011292</td>\n",
       "      <td>0.250984</td>\n",
       "      <td>0.219764</td>\n",
       "      <td>0.003832</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.202000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.186760</td>\n",
       "      <td>0.078044</td>\n",
       "      <td>0.022516</td>\n",
       "      <td>0.010352</td>\n",
       "      <td>0.421388</td>\n",
       "      <td>0.280924</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001084</td>\n",
       "      <td>0.031492</td>\n",
       "      <td>0.694532</td>\n",
       "      <td>0.097924</td>\n",
       "      <td>0.174916</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022308</td>\n",
       "      <td>0.191752</td>\n",
       "      <td>0.202116</td>\n",
       "      <td>0.491732</td>\n",
       "      <td>0.092092</td>\n",
       "      <td>0.552298</td>\n",
       "      <td>0.224659</td>\n",
       "      <td>0.735238</td>\n",
       "      <td>390.406987</td>\n",
       "      <td>4.073534</td>\n",
       "      <td>0.734297</td>\n",
       "      <td>0.358634</td>\n",
       "      <td>0.973387</td>\n",
       "      <td>0.008304</td>\n",
       "      <td>0.004724</td>\n",
       "      <td>0.004688</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.003480</td>\n",
       "      <td>0.003440</td>\n",
       "      <td>0.003528</td>\n",
       "      <td>0.004896</td>\n",
       "      <td>0.006156</td>\n",
       "      <td>0.008836</td>\n",
       "      <td>0.012608</td>\n",
       "      <td>0.020112</td>\n",
       "      <td>0.030088</td>\n",
       "      <td>0.046884</td>\n",
       "      <td>0.014552</td>\n",
       "      <td>0.025264</td>\n",
       "      <td>0.009112</td>\n",
       "      <td>0.022140</td>\n",
       "      <td>0.007452</td>\n",
       "      <td>0.007944</td>\n",
       "      <td>0.006876</td>\n",
       "      <td>0.008160</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.592148</td>\n",
       "      <td>0.133208</td>\n",
       "      <td>0.019592</td>\n",
       "      <td>17.606295</td>\n",
       "      <td>76.184733</td>\n",
       "      <td>77010.0</td>\n",
       "      <td>2147.851936</td>\n",
       "      <td>0.209772</td>\n",
       "      <td>0.774234</td>\n",
       "      <td>0.939742</td>\n",
       "      <td>0.610411</td>\n",
       "      <td>0.010120</td>\n",
       "      <td>1.575638</td>\n",
       "      <td>1.028328</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>3.874426</td>\n",
       "      <td>9.037755</td>\n",
       "      <td>0.006984</td>\n",
       "      <td>0.040538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_filename                    class_name  class_number  rgb_0_mean   rgb_0_std  rgb_0_skewness  rgb_1_mean  rgb_1_std  rgb_1_skewness  rgb_2_mean  rgb_2_std  rgb_2_skewness  hsv_0_mean  hsv_0_std  hsv_0_skewness  hsv_1_mean  hsv_1_std  hsv_1_skewness  hsv_2_mean  hsv_2_std  hsv_2_skewness  lab_0_mean  lab_0_std  lab_0_skewness  lab_1_mean  lab_1_std  lab_1_skewness  lab_2_mean  lab_2_std  lab_2_skewness  rgb_0_hist_0  rgb_0_hist_1  rgb_0_hist_2  rgb_0_hist_3  rgb_0_hist_4  rgb_0_hist_5  rgb_0_hist_6  rgb_0_hist_7  rgb_0_hist_8  rgb_0_hist_9  rgb_0_hist_10  rgb_0_hist_11  rgb_0_hist_12  rgb_0_hist_13  rgb_0_hist_14  rgb_0_hist_15  rgb_0_hist_16  rgb_0_hist_17  rgb_0_hist_18  rgb_0_hist_19  rgb_0_hist_20  rgb_0_hist_21  rgb_0_hist_22  rgb_0_hist_23  rgb_0_hist_24  rgb_0_hist_25  rgb_0_hist_26  rgb_0_hist_27  rgb_0_hist_28  rgb_0_hist_29  rgb_0_hist_30  rgb_0_hist_31  rgb_1_hist_0  rgb_1_hist_1  rgb_1_hist_2  rgb_1_hist_3  rgb_1_hist_4  rgb_1_hist_5  rgb_1_hist_6  rgb_1_hist_7  rgb_1_hist_8  rgb_1_hist_9  rgb_1_hist_10  rgb_1_hist_11  rgb_1_hist_12  rgb_1_hist_13  rgb_1_hist_14  rgb_1_hist_15  rgb_1_hist_16  rgb_1_hist_17  rgb_1_hist_18  rgb_1_hist_19  rgb_1_hist_20  rgb_1_hist_21  rgb_1_hist_22  rgb_1_hist_23  rgb_1_hist_24  rgb_1_hist_25  rgb_1_hist_26  rgb_1_hist_27  rgb_1_hist_28  rgb_1_hist_29  rgb_1_hist_30  rgb_1_hist_31  rgb_2_hist_0  rgb_2_hist_1  rgb_2_hist_2  rgb_2_hist_3  rgb_2_hist_4  rgb_2_hist_5  rgb_2_hist_6  rgb_2_hist_7  rgb_2_hist_8  rgb_2_hist_9  rgb_2_hist_10  rgb_2_hist_11  rgb_2_hist_12  rgb_2_hist_13  rgb_2_hist_14  rgb_2_hist_15  rgb_2_hist_16  rgb_2_hist_17  rgb_2_hist_18  rgb_2_hist_19  rgb_2_hist_20  rgb_2_hist_21  rgb_2_hist_22  rgb_2_hist_23  rgb_2_hist_24  rgb_2_hist_25  rgb_2_hist_26  rgb_2_hist_27  rgb_2_hist_28  rgb_2_hist_29  rgb_2_hist_30  rgb_2_hist_31  hsv_0_hist_0  hsv_0_hist_1  hsv_0_hist_2  hsv_0_hist_3  hsv_0_hist_4  hsv_0_hist_5  hsv_0_hist_6  hsv_0_hist_7  hsv_0_hist_8  hsv_0_hist_9  hsv_0_hist_10  hsv_0_hist_11  hsv_0_hist_12  hsv_0_hist_13  hsv_0_hist_14  hsv_0_hist_15  hsv_0_hist_16  hsv_0_hist_17  hsv_0_hist_18  hsv_0_hist_19  hsv_0_hist_20  hsv_0_hist_21  hsv_0_hist_22  hsv_0_hist_23  hsv_0_hist_24  hsv_0_hist_25  hsv_0_hist_26  hsv_0_hist_27  hsv_0_hist_28  hsv_0_hist_29  hsv_0_hist_30  hsv_0_hist_31  hsv_1_hist_0  hsv_1_hist_1  hsv_1_hist_2  hsv_1_hist_3  hsv_1_hist_4  hsv_1_hist_5  hsv_1_hist_6  hsv_1_hist_7  hsv_1_hist_8  hsv_1_hist_9  hsv_1_hist_10  hsv_1_hist_11  hsv_1_hist_12  hsv_1_hist_13  hsv_1_hist_14  hsv_1_hist_15  hsv_1_hist_16  hsv_1_hist_17  hsv_1_hist_18  hsv_1_hist_19  hsv_1_hist_20  hsv_1_hist_21  hsv_1_hist_22  hsv_1_hist_23  hsv_1_hist_24  hsv_1_hist_25  hsv_1_hist_26  hsv_1_hist_27  hsv_1_hist_28  hsv_1_hist_29  hsv_1_hist_30  hsv_1_hist_31  hsv_2_hist_0  hsv_2_hist_1  hsv_2_hist_2  hsv_2_hist_3  hsv_2_hist_4  hsv_2_hist_5  hsv_2_hist_6  hsv_2_hist_7  hsv_2_hist_8  hsv_2_hist_9  hsv_2_hist_10  hsv_2_hist_11  hsv_2_hist_12  hsv_2_hist_13  hsv_2_hist_14  hsv_2_hist_15  hsv_2_hist_16  hsv_2_hist_17  hsv_2_hist_18  hsv_2_hist_19  hsv_2_hist_20  hsv_2_hist_21  hsv_2_hist_22  hsv_2_hist_23  hsv_2_hist_24  hsv_2_hist_25  hsv_2_hist_26  hsv_2_hist_27  hsv_2_hist_28  hsv_2_hist_29  hsv_2_hist_30  hsv_2_hist_31  lab_0_hist_0  lab_0_hist_1  lab_0_hist_2  lab_0_hist_3  lab_0_hist_4  lab_0_hist_5  lab_0_hist_6  lab_0_hist_7  lab_0_hist_8  lab_0_hist_9  lab_0_hist_10  lab_0_hist_11  lab_0_hist_12  lab_0_hist_13  lab_0_hist_14  lab_0_hist_15  lab_0_hist_16  lab_0_hist_17  lab_0_hist_18  lab_0_hist_19  lab_0_hist_20  lab_0_hist_21  lab_0_hist_22  lab_0_hist_23  lab_0_hist_24  lab_0_hist_25  lab_0_hist_26  lab_0_hist_27  lab_0_hist_28  lab_0_hist_29  lab_0_hist_30  lab_0_hist_31  lab_1_hist_0  lab_1_hist_1  lab_1_hist_2  lab_1_hist_3  lab_1_hist_4  lab_1_hist_5  lab_1_hist_6  lab_1_hist_7  lab_1_hist_8  lab_1_hist_9  lab_1_hist_10  lab_1_hist_11  lab_1_hist_12  lab_1_hist_13  lab_1_hist_14  lab_1_hist_15  lab_1_hist_16  lab_1_hist_17  lab_1_hist_18  lab_1_hist_19  lab_1_hist_20  lab_1_hist_21  lab_1_hist_22  lab_1_hist_23  lab_1_hist_24  lab_1_hist_25  lab_1_hist_26  lab_1_hist_27  lab_1_hist_28  lab_1_hist_29  lab_1_hist_30  lab_1_hist_31  lab_2_hist_0  lab_2_hist_1  lab_2_hist_2  lab_2_hist_3  lab_2_hist_4  lab_2_hist_5  lab_2_hist_6  lab_2_hist_7  lab_2_hist_8  lab_2_hist_9  lab_2_hist_10  lab_2_hist_11  lab_2_hist_12  lab_2_hist_13  lab_2_hist_14  lab_2_hist_15  lab_2_hist_16  lab_2_hist_17  lab_2_hist_18  lab_2_hist_19  lab_2_hist_20  lab_2_hist_21  lab_2_hist_22  lab_2_hist_23  lab_2_hist_24  lab_2_hist_25  lab_2_hist_26  lab_2_hist_27  lab_2_hist_28  lab_2_hist_29  lab_2_hist_30  lab_2_hist_31  dominant_color_0_ratio  dominant_color_1_ratio  dominant_color_2_ratio  dominant_color_3_ratio  dominant_color_4_ratio  green_ratio  saturation_ratio  value_ratio  glcm_contrast  glcm_dissimilarity  glcm_homogeneity  glcm_energy  glcm_correlation  lbp_hist_0  lbp_hist_1  lbp_hist_2  lbp_hist_3  lbp_hist_4  lbp_hist_5  lbp_hist_6  lbp_hist_7  lbp_hist_8  lbp_hist_9  lbp_hist_10  lbp_hist_11  lbp_hist_12  lbp_hist_13  lbp_hist_14  lbp_hist_15  lbp_hist_16  lbp_hist_17  lbp_hist_18  lbp_hist_19  lbp_hist_20  lbp_hist_21  lbp_hist_22  lbp_hist_23  lbp_hist_24  lbp_hist_25  edge_density  roughness_mean  roughness_std      area    perimeter  circularity  eccentricity  solidity    extent  vein_density  mean_orientation  orientation_std  branching_density  mean_segment_length  segment_length_std  vesselness_mean  vesselness_std\n",
       "0  Class1_19.webp  1Hibiscus rosa-sinensis(HRS)             1  188.705416   69.990575       -1.354648  200.608360  50.907016       -1.110815  196.463884  58.276169       -1.239846   63.958588  36.265557       -1.122727   27.474816  55.044592        1.993572  200.732288  50.759808       -1.110568  202.662404  51.991219       -1.237514  122.988884   9.028771       -1.966733  129.199640   3.215881        1.895660      0.000028      0.001372      0.006260      0.026896      0.046820      0.041008      0.017992      0.008400      0.004956      0.004536       0.004756       0.004272       0.003732       0.002320       0.001872       0.001232       0.000932       0.000912       0.000768       0.001128       0.001188       0.001156       0.001904       0.003572       0.051484       0.353044       0.155904       0.001548       0.000008       0.000000       0.000064       0.249936           0.0           0.0           0.0           0.0       0.00002      0.000248      0.001252      0.000644      0.001432      0.005632       0.014448       0.032056       0.044912       0.053304       0.013524       0.003932       0.002300       0.001852       0.001488       0.001052       0.001068       0.001124       0.001268       0.001868       0.011324       0.282180       0.251576       0.021156       0.000336       0.000004       0.000000       0.250000           0.0           0.0       0.00000      0.000004      0.000092      0.001152      0.004020      0.010832      0.033844      0.052064       0.041012       0.012548       0.006632       0.004388       0.003692       0.002264       0.002000       0.001984       0.000920       0.001084       0.001084       0.001176       0.001388       0.002328       0.019492       0.305468       0.232788       0.007572       0.000172       0.000000       0.000000       0.250000      0.237140      0.000036      0.000136      0.000340      0.000112      0.001640      0.001376      0.009608      0.015672      0.154988       0.352220       0.197080       0.018640       0.008592       0.001568       0.000056       0.000544       0.000000       0.000164       0.000016       0.000024       0.000044       0.000004            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0      0.791756      0.025052      0.004524      0.005696      0.008972      0.003800      0.002740      0.001960      0.001812      0.001936       0.001620       0.002016       0.002644       0.003524       0.005400       0.006768       0.009632       0.010944       0.013372       0.021424       0.026444       0.022620       0.013632       0.006224       0.003032       0.001568       0.000736       0.000112       0.000012       0.000008       0.000004       0.000016           0.0           0.0           0.0           0.0       0.00002      0.000064      0.001188      0.000788      0.001136      0.005460       0.014312       0.032092       0.044452       0.053112       0.013652       0.003840       0.002376       0.002716       0.001464       0.001256       0.001132       0.001060       0.001236       0.001948       0.010696       0.273740       0.260160       0.021760       0.000336       0.000004       0.000000       0.250000           0.0           0.0           0.0      0.000004      0.000036      0.000112      0.001264      0.000744      0.002360      0.007704       0.019512       0.036740       0.049760       0.039172       0.009180       0.003612       0.002928       0.002060       0.001384       0.001060       0.000944       0.001064       0.001228       0.001572       0.002300       0.040524       0.371596       0.151488       0.001648       0.000004       0.000000       0.250000           0.0           0.0           0.0           0.0           0.0           0.0           0.0           0.0           0.0           0.0       0.000000       0.000168       0.113476       0.031416       0.013176       0.593596       0.248168            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0           0.0           0.0           0.0           0.0           0.0           0.0           0.0           0.0           0.0           0.0            0.0            0.0            0.0       0.000008       0.001148       0.043396       0.846508       0.108940       0.000000       0.000000            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0                0.568708                0.143312                0.250000                0.029140                0.008840     0.520832          0.107744     0.787185     390.718182            3.068500          0.855175     0.592378          0.977475    0.003560    0.002832    0.002148    0.002052    0.001840    0.002108    0.002072    0.002944    0.003592    0.005068     0.006868     0.010460     0.014336     0.021124     0.008388     0.016320     0.005696     0.011304     0.004544     0.005596     0.003724     0.005488     0.003476     0.002592     0.781676     0.070192      0.012512       12.934366      77.796735   40741.0  1735.602155     0.169958      0.736233  0.914860  0.638294      0.009984          1.570723         1.028328           0.000068             4.333333            8.720785         0.010135        0.060742\n",
       "1  Class1_93.webp  1Hibiscus rosa-sinensis(HRS)             1  149.297972  108.692582       -0.315699  181.381852  71.554596       -0.283444  162.624352  95.863944       -0.331065   67.937152  46.860072       -0.202374   85.817992  99.754710        0.489063  182.864468  72.367514       -0.305200  180.119244  74.785723       -0.301763  114.000272  17.040622       -0.363435  136.101280  11.361412        0.416876      0.072368      0.112692      0.087504      0.055740      0.024880      0.015100      0.010480      0.008300      0.006988      0.005736       0.004588       0.004116       0.003956       0.004132       0.004020       0.004412       0.003628       0.004860       0.002940       0.001216       0.000812       0.000852       0.000824       0.001104       0.001052       0.001532       0.002100       0.009324       0.062896       0.179048       0.052552       0.250248           0.0           0.0           0.0           0.0       0.00000      0.000000      0.000000      0.000004      0.000920      0.007032       0.063328       0.130272       0.090976       0.056196       0.029280       0.015476       0.010436       0.009424       0.008280       0.009176       0.003012       0.001920       0.001388       0.001036       0.001208       0.001244       0.001644       0.004576       0.107056       0.156856       0.039188       0.250072           0.0           0.0       0.00002      0.040384      0.096580      0.071772      0.069432      0.053628      0.029980      0.016832       0.010784       0.007536       0.006336       0.005104       0.003948       0.003432       0.003036       0.002464       0.002484       0.002548       0.005112       0.005208       0.001088       0.001072       0.001080       0.001240       0.001488       0.003224       0.034200       0.158528       0.111376       0.250084      0.259416      0.000332      0.000040      0.005956      0.000056      0.001548      0.000212      0.004004      0.282264      0.134908       0.010520       0.018860       0.007380       0.050316       0.017316       0.042160       0.149356       0.002768       0.007136       0.002192       0.002156       0.001040       0.000064            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0      0.477616      0.075800      0.004976      0.002496      0.004316      0.008676      0.003908      0.003388      0.003620      0.004352       0.004604       0.003636       0.004416       0.004496       0.004976       0.005596       0.005944       0.006328       0.007352       0.007836       0.010536       0.012560       0.018940       0.026104       0.036088       0.036528       0.041420       0.042212       0.045968       0.036836       0.029892       0.018584           0.0           0.0           0.0           0.0       0.00000      0.000000      0.000000      0.000004      0.000920      0.007032       0.063316       0.130180       0.090936       0.056232       0.028784       0.014652       0.009968       0.007388       0.006532       0.005572       0.007104       0.006260       0.001624       0.001124       0.001136       0.001336       0.001400       0.002668       0.034228       0.157240       0.114012       0.250352           0.0           0.0           0.0      0.000000      0.000000      0.000000      0.000000      0.000236      0.003436      0.039404       0.124684       0.101328       0.065192       0.034756       0.016704       0.011012       0.008412       0.007284       0.007376       0.006992       0.006988       0.001944       0.001228       0.001064       0.001132       0.001268       0.001460       0.002652       0.041552       0.135612       0.128212       0.250072           0.0           0.0           0.0           0.0           0.0           0.0           0.0           0.0           0.0           0.0       0.000492       0.321072       0.081688       0.015808       0.005704       0.045904       0.529332            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0           0.0           0.0           0.0           0.0           0.0           0.0           0.0           0.0           0.0           0.0            0.0            0.0            0.0       0.000000       0.011848       0.271148       0.303512       0.028516       0.251820       0.133156            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0                0.314116                0.310596                0.090152                0.250296                0.034840     0.581497          0.336541     0.717116     582.706028            5.611343          0.652550     0.338181          0.956031    0.012568    0.007900    0.006644    0.005724    0.005732    0.005584    0.005540    0.008140    0.009216    0.012144     0.016048     0.022756     0.030440     0.041632     0.017488     0.030060     0.012420     0.022140     0.011180     0.012556     0.010404     0.012468     0.010268     0.008288     0.490324     0.172336      0.035880       24.287410      92.470516  110328.5  2543.148399     0.214365      0.575504  0.940362  0.674998      0.007540          1.559771         1.028328           0.000248             2.429124            2.528218         0.006045        0.039673\n",
       "2   Class1_3.webp  1Hibiscus rosa-sinensis(HRS)             1  156.211568  103.263263       -0.424134  186.153564  66.936745       -0.355221  173.055608  84.571048       -0.401460   72.805396  48.344912       -0.277568   75.240344  93.881327        0.737977  187.413092  67.553960       -0.373898  185.460908  69.723658       -0.380676  115.813816  15.422224       -0.494102  132.942916   7.584343        0.511862      0.066928      0.091884      0.049820      0.030564      0.025980      0.024468      0.020172      0.016252      0.014020      0.011656       0.009728       0.008448       0.006960       0.008636       0.011040       0.010324       0.009260       0.005492       0.002712       0.001352       0.001032       0.000876       0.000772       0.001044       0.001104       0.001484       0.002500       0.022636       0.090500       0.170484       0.031800       0.250072           0.0           0.0           0.0           0.0       0.00000      0.000000      0.000000      0.000000      0.000004      0.000424       0.020828       0.085036       0.091808       0.078008       0.043656       0.024228       0.021992       0.018128       0.012452       0.008108       0.008900       0.006724       0.003988       0.002092       0.001248       0.001188       0.001588       0.005748       0.128988       0.182884       0.001968       0.250012           0.0           0.0       0.00000      0.000000      0.000056      0.019236      0.083580      0.064548      0.068648      0.047260       0.031932       0.021608       0.014948       0.012044       0.008684       0.007652       0.008776       0.012412       0.011656       0.007552       0.003532       0.001236       0.000952       0.000996       0.000968       0.001156       0.001640       0.004376       0.068936       0.185504       0.060108       0.250004      0.246892      0.000348      0.000140      0.004668      0.000116      0.000716      0.000280      0.001464      0.030120      0.366952       0.020760       0.014824       0.023364       0.052192       0.015388       0.055212       0.089752       0.010220       0.048488       0.011976       0.004488       0.001512       0.000128            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0      0.533820      0.033408      0.003636      0.002700      0.006416      0.007780      0.012776      0.009956      0.007648      0.007236       0.008396       0.009372       0.009732       0.010556       0.010740       0.011952       0.011368       0.012724       0.013092       0.013008       0.011140       0.011312       0.011540       0.011440       0.013652       0.017840       0.025464       0.033920       0.039196       0.046096       0.030152       0.011932           0.0           0.0           0.0           0.0       0.00000      0.000000      0.000000      0.000000      0.000004      0.000408       0.020788       0.085040       0.091848       0.077924       0.043600       0.023944       0.016056       0.016404       0.014572       0.011640       0.010816       0.006804       0.004072       0.002112       0.001216       0.001224       0.001292       0.003416       0.068564       0.180272       0.067868       0.250116           0.0           0.0           0.0      0.000000      0.000000      0.000000      0.000000      0.000000      0.000080      0.006692       0.071212       0.089240       0.081572       0.052668       0.029288       0.016796       0.016764       0.016932       0.013220       0.010564       0.009156       0.006620       0.003772       0.001720       0.001172       0.001232       0.001360       0.002732       0.064084       0.196512       0.056604       0.250008           0.0           0.0           0.0           0.0           0.0           0.0           0.0           0.0           0.0           0.0       0.000000       0.188476       0.153000       0.058592       0.009068       0.061776       0.529088            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0           0.0           0.0           0.0           0.0           0.0           0.0           0.0           0.0           0.0           0.0            0.0            0.0            0.0       0.000000       0.004680       0.309604       0.295860       0.247032       0.142824       0.000000            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0                0.107552                0.250028                0.064836                0.322872                0.254712     0.565357          0.295060     0.734953     554.150002            5.658388          0.647332     0.302098          0.961747    0.013300    0.006572    0.006348    0.006048    0.005696    0.005600    0.005652    0.007840    0.009036    0.011928     0.016192     0.023004     0.030160     0.050296     0.018328     0.029028     0.012716     0.026176     0.009832     0.010868     0.009296     0.010044     0.009140     0.006856     0.486776     0.173268      0.039868       24.471842      89.425351  106122.0  2715.793061     0.180810      0.576994  0.928529  0.625008      0.008500          1.574884         1.028328           0.000268             2.403846            2.498603         0.006453        0.038890\n",
       "3  Class1_37.webp  1Hibiscus rosa-sinensis(HRS)             1  163.233416   94.049086       -0.711829  189.664052  59.073714       -0.575906  177.131960  76.471072       -0.680517   60.176716  36.350931       -0.938488   61.858284  90.301038        0.955800  189.780572  59.030649       -0.579672  189.826872  61.983841       -0.642626  117.096448  15.298257       -0.907872  132.833328   7.749605        1.023750      0.019168      0.062048      0.085756      0.063616      0.028804      0.014560      0.009788      0.007616      0.004916      0.003936       0.003288       0.001876       0.001532       0.001268       0.002432       0.001576       0.000884       0.000744       0.000824       0.000780       0.000880       0.001088       0.001408       0.002236       0.005404       0.181904       0.232596       0.009052       0.000020       0.000000       0.000000       0.250000           0.0           0.0           0.0           0.0       0.00000      0.000000      0.000000      0.000000      0.000744      0.004376       0.023948       0.041152       0.053784       0.074480       0.067128       0.025040       0.009140       0.004576       0.003472       0.003300       0.001896       0.001116       0.000964       0.001172       0.001604       0.035580       0.309460       0.086812       0.000248       0.000008       0.000000       0.250000           0.0           0.0       0.00000      0.000056      0.007796      0.023912      0.041636      0.062716      0.070116      0.051140       0.021180       0.009976       0.006548       0.004872       0.002928       0.002884       0.002304       0.002672       0.001688       0.000912       0.000848       0.001032       0.001004       0.001472       0.003272       0.057152       0.342952       0.028928       0.000004       0.000000       0.000000       0.250000      0.258388      0.000228      0.000140      0.000372      0.000124      0.001864      0.001496      0.004880      0.015340      0.304580       0.223872       0.160100       0.014940       0.009124       0.002156       0.000236       0.001280       0.000024       0.000228       0.000064       0.000076       0.000452       0.000036            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0      0.660348      0.017024      0.004024      0.003252      0.003276      0.004044      0.003408      0.002228      0.001356      0.001384       0.001424       0.001416       0.001640       0.001832       0.002244       0.002768       0.004324       0.005836       0.007068       0.006832       0.008316       0.011336       0.015628       0.026448       0.035076       0.043076       0.042344       0.033308       0.021424       0.018012       0.007080       0.002224           0.0           0.0           0.0           0.0       0.00000      0.000000      0.000000      0.000000      0.000744      0.004376       0.023900       0.041116       0.053572       0.074560       0.066880       0.023336       0.008664       0.006076       0.004328       0.003356       0.001916       0.001132       0.000976       0.001184       0.001512       0.032452       0.311164       0.088484       0.000264       0.000008       0.000000       0.250000           0.0           0.0           0.0      0.000000      0.000000      0.000000      0.000000      0.000080      0.002448      0.011332       0.040244       0.048532       0.071448       0.076992       0.031552       0.011832       0.006924       0.004872       0.003568       0.002144       0.001048       0.000964       0.000916       0.001172       0.001452       0.003240       0.214140       0.214796       0.000304       0.000000       0.000000       0.250000           0.0           0.0           0.0           0.0           0.0           0.0           0.0           0.0           0.0           0.0       0.000128       0.236076       0.048932       0.013752       0.009636       0.419376       0.272100            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0           0.0           0.0           0.0           0.0           0.0           0.0           0.0           0.0           0.0           0.0            0.0            0.0            0.0       0.000000       0.001676       0.034356       0.676472       0.084720       0.202016       0.000760            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0                0.146292                0.435780                0.250000                0.144488                0.023440     0.557237          0.242582     0.744238     449.586281            4.396186          0.713631     0.388460          0.969847    0.008004    0.004972    0.004584    0.004404    0.004064    0.004180    0.004080    0.006056    0.007820    0.010220     0.013612     0.022296     0.032780     0.046636     0.015808     0.022632     0.009900     0.022732     0.007136     0.010144     0.006820     0.008192     0.006892     0.004988     0.568852     0.142196      0.018328       18.605150      80.847696   82218.0  2747.592050     0.136859      0.648862  0.867059  0.564130      0.012176          1.579425         1.028328           0.000292             4.147139           10.222332         0.010463        0.060133\n",
       "4  Class1_80.webp  1Hibiscus rosa-sinensis(HRS)             1  163.510904   90.259988       -0.751666  187.378692  58.149150       -0.605425  175.759856  73.987816       -0.703206   56.812688  35.514640       -0.738473   57.288048  85.206538        1.021426  187.485608  58.089104       -0.606174  188.002652  60.970680       -0.674752  117.896696  14.371689       -0.977628  132.454396   7.107394        1.096428      0.005116      0.042412      0.091056      0.065188      0.032408      0.023008      0.013552      0.008960      0.005112      0.003032       0.004048       0.003640       0.002832       0.001748       0.001184       0.000856       0.000752       0.000832       0.000844       0.001032       0.001184       0.001436       0.001880       0.002776       0.010252       0.177144       0.290696       0.004696       0.000244       0.000076       0.000004       0.202000           0.0           0.0           0.0           0.0       0.00000      0.000008      0.000132      0.001044      0.001804      0.002912       0.022892       0.041884       0.081932       0.077772       0.044964       0.015544       0.006144       0.003492       0.001996       0.001668       0.001056       0.001136       0.001356       0.002016       0.004480       0.071408       0.306208       0.104812       0.001088       0.000200       0.000052       0.202000           0.0           0.0       0.00000      0.000548      0.002920      0.017088      0.038312      0.069784      0.081712      0.041920       0.017820       0.010928       0.008488       0.005668       0.004008       0.002536       0.001264       0.000924       0.000848       0.000956       0.001024       0.001332       0.001900       0.002524       0.007224       0.108740       0.304952       0.063476       0.000928       0.000124       0.000056       0.201996      0.253584      0.001888      0.001640      0.007656      0.000500      0.048972      0.003116      0.031012      0.035520      0.300548       0.189468       0.100084       0.010480       0.011356       0.000632       0.001036       0.001120       0.000000       0.000100       0.000012       0.000152       0.001108       0.000016            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0      0.662276      0.025048      0.004000      0.003652      0.005744      0.003632      0.001596      0.001420      0.001196      0.001072       0.001284       0.002088       0.002888       0.002976       0.003800       0.005152       0.005592       0.007808       0.009068       0.012236       0.012496       0.014988       0.019088       0.027644       0.035540       0.046980       0.038008       0.024772       0.010588       0.004744       0.001768       0.000856           0.0           0.0           0.0           0.0       0.00000      0.000004      0.000116      0.000940      0.001604      0.002576       0.022864       0.042004       0.081280       0.077624       0.045492       0.016060       0.006360       0.003548       0.001980       0.001672       0.001080       0.001124       0.001352       0.002012       0.004264       0.069828       0.307020       0.105844       0.001096       0.000204       0.000052       0.202000           0.0           0.0           0.0      0.000000      0.000000      0.000028      0.000232      0.001652      0.002060      0.010264       0.031560       0.073020       0.083828       0.056660       0.025412       0.009212       0.004784       0.002468       0.001568       0.001400       0.000896       0.001068       0.001204       0.001792       0.002704       0.011292       0.250984       0.219764       0.003832       0.000244       0.000072       0.202000           0.0           0.0           0.0           0.0           0.0           0.0           0.0           0.0           0.0           0.0       0.000016       0.186760       0.078044       0.022516       0.010352       0.421388       0.280924            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0           0.0           0.0           0.0           0.0           0.0           0.0           0.0           0.0           0.0           0.0            0.0            0.0            0.0       0.000000       0.001084       0.031492       0.694532       0.097924       0.174916       0.000052            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0            0.0                0.022308                0.191752                0.202116                0.491732                0.092092     0.552298          0.224659     0.735238     390.406987            4.073534          0.734297     0.358634          0.973387    0.008304    0.004724    0.004688    0.003600    0.003480    0.003440    0.003528    0.004896    0.006156    0.008836     0.012608     0.020112     0.030088     0.046884     0.014552     0.025264     0.009112     0.022140     0.007452     0.007944     0.006876     0.008160     0.006800     0.005000     0.592148     0.133208      0.019592       17.606295      76.184733   77010.0  2147.851936     0.209772      0.774234  0.939742  0.610411      0.010120          1.575638         1.028328           0.000256             3.874426            9.037755         0.006984        0.040538"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.utils_data import FEATURE_DATA_FOLDER\n",
    "\n",
    "feature_dataset_path = FEATURE_DATA_FOLDER / \"feature_dataset.csv\"\n",
    "feature_dataset = pd.read_csv(feature_dataset_path)\n",
    "feature_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (3976, 371)\n",
      "Testing set shape: (995, 371)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Remove non-feature columns\n",
    "X = feature_dataset.drop(columns=[\"class_name\", \"class_number\", \"image_filename\"])\n",
    "\n",
    "y = feature_dataset[\"class_number\"]\n",
    "\n",
    "# Split into train and test sets (80-20 split with stratification)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from src.utils_classical import compare_classification_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed KNN: Mean Accuracy = 0.9135\n",
      "Completed MLP: Mean Accuracy = 0.9691\n",
      "Completed Random Forest: Mean Accuracy = 0.9721\n",
      "\n",
      "Model Comparison Results:\n"
     ]
    }
   ],
   "source": [
    "# Create pipelines with StandardScaler for each model\n",
    "models = {\n",
    "    \"KNN\": Pipeline(\n",
    "        [(\"scaler\", StandardScaler()), (\"classifier\", KNeighborsClassifier())]\n",
    "    ),\n",
    "    \"MLP\": Pipeline(\n",
    "        [\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\n",
    "                \"classifier\",\n",
    "                MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42),\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    "    \"Random Forest\": Pipeline(\n",
    "        [\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"classifier\", RandomForestClassifier(random_state=42)),\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Compare models with cross validation\n",
    "results = compare_classification_models(X_train, y_train - 1, models)\n",
    "\n",
    "print(\"\\nModel Comparison Results:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Model         |   Accuracy |\n",
      "|:--------------|-----------:|\n",
      "| Random Forest |   0.972083 |\n",
      "| MLP           |   0.969063 |\n",
      "| KNN           |   0.913482 |\n"
     ]
    }
   ],
   "source": [
    "print(results.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost + dimensionality reduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from src.utils_autoencoder import (\n",
    "    SemiSupervisedAutoencoder,\n",
    "    create_dataloader,\n",
    "    full_X_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder = SemiSupervisedAutoencoder(\n",
    "    input_dim=X_train.shape[1],\n",
    "    encoding_dim=50,\n",
    "    num_classes=len(set(y_train)),\n",
    ")\n",
    "\n",
    "autoencoder.autoencoder.load_state_dict(torch.load(\"../models/autoencoder_weights.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features shape: torch.Size([4971, 50])\n",
      "Latent features shape: torch.Size([4971, 50])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "full_data_loader = full_X_dataloader(X_scaled)\n",
    "reduced_features = autoencoder.extract_features(full_data_loader)\n",
    "\n",
    "print(\"Latent features shape:\", reduced_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed XGBoost + PCA: Mean Accuracy = 0.9006\n",
      "Completed XGBoost + LDA: Mean Accuracy = 0.9491\n",
      "Completed XGBoost + Autoencoder: Mean Accuracy = 0.9165\n",
      "\n",
      "Final Results:\n",
      "| Model                 |   Accuracy |\n",
      "|:----------------------|-----------:|\n",
      "| XGBoost + LDA         |   0.949104 |\n",
      "| XGBoost + PCA         |   0.900624 |\n",
      "| XGBoost + Autoencoder |   0.916514 |\n",
      "| Model                 |   Accuracy |\n",
      "|:----------------------|-----------:|\n",
      "| XGBoost + LDA         |   0.949104 |\n",
      "| XGBoost + PCA         |   0.900624 |\n",
      "| XGBoost + Autoencoder |   0.916514 |\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from src.utils_classical import compare_classification_models\n",
    "\n",
    "# Initialize XGBoost classifier\n",
    "xgb_clf = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "# Pipeline 1: PCA\n",
    "pca_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"pca\", PCA(n_components=50)),\n",
    "        (\"classifier\", xgb_clf),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Pipeline 2: LDA\n",
    "lda_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"lda\", LinearDiscriminantAnalysis()),\n",
    "        (\"classifier\", xgb_clf),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Pipeline 3: Pre-trained Autoencoder features\n",
    "# Note: Features are already reduced, just need classifier\n",
    "ae_pipeline = Pipeline([(\"classifier\", xgb.XGBClassifier(random_state=42))])\n",
    "\n",
    "# Dictionary of models to compare\n",
    "models = {\n",
    "    \"XGBoost + PCA\": pca_pipeline,\n",
    "    \"XGBoost + LDA\": lda_pipeline,\n",
    "    \"XGBoost + Autoencoder\": ae_pipeline,\n",
    "}\n",
    "\n",
    "# Compare models with PCA and LDA using original features\n",
    "results_pca_lda = compare_classification_models(\n",
    "    X, y - 1, {k: v for k, v in models.items() if k != \"XGBoost + Autoencoder\"}\n",
    ")\n",
    "\n",
    "# Compare model with autoencoder features\n",
    "results_ae = compare_classification_models(\n",
    "    reduced_features, y - 1, {\"XGBoost + Autoencoder\": models[\"XGBoost + Autoencoder\"]}\n",
    ")\n",
    "\n",
    "# Combine results\n",
    "final_results = pd.concat([results_pca_lda, results_ae])\n",
    "print(\"\\nFinal Results:\")\n",
    "print(final_results.to_markdown(index=False))  # # # # # # # #\n",
    "print(final_results.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna Hyperparameter Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-14 19:23:03,839] A new study created in memory with name: no-name-a55f8ee8-5a89-4fde-8526-dd0154b1d73a\n",
      "[I 2024-12-14 19:24:09,487] Trial 0 finished with value: 0.9674088753627291 and parameters: {'n_estimators': 200, 'max_depth': 7, 'learning_rate': 0.21682215798085835, 'subsample': 0.6387532181170827, 'colsample_bytree': 0.921632179288466, 'min_child_weight': 1, 'gamma': 0.26929906077232874}. Best is trial 0 with value: 0.9674088753627291.\n",
      "[I 2024-12-14 19:25:27,342] Trial 1 finished with value: 0.9706285956947717 and parameters: {'n_estimators': 300, 'max_depth': 7, 'learning_rate': 0.0972328107918768, 'subsample': 0.8104763890019653, 'colsample_bytree': 0.8155047101431188, 'min_child_weight': 7, 'gamma': 0.4933418982853346}. Best is trial 1 with value: 0.9706285956947717.\n",
      "[I 2024-12-14 19:26:24,249] Trial 2 finished with value: 0.9742497194220601 and parameters: {'n_estimators': 300, 'max_depth': 3, 'learning_rate': 0.2253641204685649, 'subsample': 0.7185234979269632, 'colsample_bytree': 0.8258000332640414, 'min_child_weight': 5, 'gamma': 0.07689510628659096}. Best is trial 2 with value: 0.9742497194220601.\n",
      "[I 2024-12-14 19:27:21,927] Trial 3 finished with value: 0.9712318129884837 and parameters: {'n_estimators': 300, 'max_depth': 9, 'learning_rate': 0.22191505701249187, 'subsample': 0.9978938425613807, 'colsample_bytree': 0.6054194286412483, 'min_child_weight': 5, 'gamma': 0.2965979354457986}. Best is trial 2 with value: 0.9742497194220601.\n",
      "[I 2024-12-14 19:28:07,530] Trial 4 finished with value: 0.9718362435922066 and parameters: {'n_estimators': 200, 'max_depth': 9, 'learning_rate': 0.2232187559738948, 'subsample': 0.6112936098388074, 'colsample_bytree': 0.8242835191897414, 'min_child_weight': 5, 'gamma': 0.3888088293855347}. Best is trial 2 with value: 0.9742497194220601.\n",
      "[I 2024-12-14 19:31:45,337] Trial 5 finished with value: 0.977065609738835 and parameters: {'n_estimators': 300, 'max_depth': 9, 'learning_rate': 0.035151634486733765, 'subsample': 0.7921265648741315, 'colsample_bytree': 0.6903310371650624, 'min_child_weight': 1, 'gamma': 0.15267999899982498}. Best is trial 5 with value: 0.977065609738835.\n",
      "[I 2024-12-14 19:31:45,339] A new study created in memory with name: no-name-334220fd-2d1c-452d-9c5d-a82a3cacf3b8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best trial (Original Data + StandardScaler):\n",
      "Value: 0.9771\n",
      "Best hyperparameters: {'n_estimators': 300, 'max_depth': 9, 'learning_rate': 0.035151634486733765, 'subsample': 0.7921265648741315, 'colsample_bytree': 0.6903310371650624, 'min_child_weight': 1, 'gamma': 0.15267999899982498}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-14 19:32:17,548] Trial 0 finished with value: 0.9589612044124041 and parameters: {'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.04224842130280718, 'subsample': 0.6768985372773917, 'colsample_bytree': 0.6497082185568657, 'min_child_weight': 1, 'gamma': 0.23415373413990392}. Best is trial 0 with value: 0.9589612044124041.\n",
      "[I 2024-12-14 19:32:31,406] Trial 1 finished with value: 0.9535298221489743 and parameters: {'n_estimators': 300, 'max_depth': 9, 'learning_rate': 0.11038426548858894, 'subsample': 0.8769673736707733, 'colsample_bytree': 0.7960320128325131, 'min_child_weight': 3, 'gamma': 0.3210516091250368}. Best is trial 0 with value: 0.9589612044124041.\n",
      "[I 2024-12-14 19:32:39,784] Trial 2 finished with value: 0.9507131229588588 and parameters: {'n_estimators': 100, 'max_depth': 7, 'learning_rate': 0.054359672580592495, 'subsample': 0.6163997951121263, 'colsample_bytree': 0.6691014445912565, 'min_child_weight': 7, 'gamma': 0.2658607142040738}. Best is trial 0 with value: 0.9589612044124041.\n",
      "[I 2024-12-14 19:32:55,770] Trial 3 finished with value: 0.9553390695934401 and parameters: {'n_estimators': 500, 'max_depth': 5, 'learning_rate': 0.17395900564005892, 'subsample': 0.6834311721787412, 'colsample_bytree': 0.6081089214373925, 'min_child_weight': 7, 'gamma': 0.43517533566404054}. Best is trial 0 with value: 0.9589612044124041.\n",
      "[I 2024-12-14 19:33:16,403] Trial 4 finished with value: 0.9545354539296078 and parameters: {'n_estimators': 500, 'max_depth': 7, 'learning_rate': 0.10890829560913931, 'subsample': 0.6816803790533476, 'colsample_bytree': 0.7173175810043739, 'min_child_weight': 1, 'gamma': 0.37875356948915234}. Best is trial 0 with value: 0.9589612044124041.\n",
      "[I 2024-12-14 19:33:46,824] Trial 5 finished with value: 0.9565465152725399 and parameters: {'n_estimators': 500, 'max_depth': 5, 'learning_rate': 0.026766601124949822, 'subsample': 0.9052069541198353, 'colsample_bytree': 0.6364484729287977, 'min_child_weight': 5, 'gamma': 0.4558764757925933}. Best is trial 0 with value: 0.9589612044124041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best trial (StandardScaler + LDA):\n",
      "Value: 0.9590\n",
      "Best hyperparameters: {'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.04224842130280718, 'subsample': 0.6768985372773917, 'colsample_bytree': 0.6497082185568657, 'min_child_weight': 1, 'gamma': 0.23415373413990392}\n",
      "\n",
      "Final Results after Hyperparameter Optimization:\n",
      "| Model                                |   Accuracy |\n",
      "|:-------------------------------------|-----------:|\n",
      "| XGBoost + StandardScaler (Optimized) |   0.977066 |\n",
      "| XGBoost + LDA (Optimized)            |   0.958961 |\n"
     ]
    },
    {
     "ename": "NotFittedError",
     "evalue": "This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 94\u001b[0m\n\u001b[1;32m     91\u001b[0m best_lda_model \u001b[38;5;241m=\u001b[39m study_lda\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39muser_attrs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_lda_model)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Generate predictions using the optimized models\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m base_preds \u001b[38;5;241m=\u001b[39m \u001b[43mbest_base_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m lda_preds \u001b[38;5;241m=\u001b[39m best_lda_model\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Plot confusion matrices\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/upd_ai_201/lib/python3.12/site-packages/sklearn/pipeline.py:602\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[0;34m(self, X, **params)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _routing_enabled():\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 602\u001b[0m         Xt \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    605\u001b[0m \u001b[38;5;66;03m# metadata routing enabled\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/upd_ai_201/lib/python3.12/site-packages/sklearn/utils/_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    301\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/upd_ai_201/lib/python3.12/site-packages/sklearn/preprocessing/_data.py:1040\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform standardization by centering and scaling.\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m \n\u001b[1;32m   1028\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;124;03m        Transformed array.\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1040\u001b[0m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1042\u001b[0m     copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m   1043\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m   1044\u001b[0m         X,\n\u001b[1;32m   1045\u001b[0m         reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1050\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/upd_ai_201/lib/python3.12/site-packages/sklearn/utils/validation.py:1622\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1619\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not an estimator instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (estimator))\n\u001b[1;32m   1621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001b[0;32m-> 1622\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from src.utils_classical import generate_objective\n",
    "\n",
    "\n",
    "# Define search spaces for XGBoost hyperparameters\n",
    "xgb_search_space = {\n",
    "    \"n_estimators\": [100, 200, 300, 500],\n",
    "    \"max_depth\": [3, 5, 7, 9],\n",
    "    \"learning_rate\": (0.01, 0.3),\n",
    "    \"subsample\": (0.6, 1.0),\n",
    "    \"colsample_bytree\": (0.6, 1.0),\n",
    "    \"min_child_weight\": [1, 3, 5, 7],\n",
    "    \"gamma\": (0, 0.5),\n",
    "}\n",
    "\n",
    "# Pipeline for original data + standard scaler\n",
    "base_pipeline = Pipeline(\n",
    "    [(\"scaler\", StandardScaler()), (\"classifier\", xgb.XGBClassifier(random_state=42))]\n",
    ")\n",
    "\n",
    "# Pipeline for standard scaler + LDA\n",
    "lda_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"lda\", LinearDiscriminantAnalysis()),\n",
    "        (\"classifier\", xgb.XGBClassifier(random_state=42)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create study for original data pipeline\n",
    "study_base = optuna.create_study(direction=\"maximize\")\n",
    "objective_base = generate_objective(X, y - 1, base_pipeline, xgb_search_space)\n",
    "study_base.optimize(objective_base, n_trials=6)\n",
    "\n",
    "print(\"\\nBest trial (Original Data + StandardScaler):\")\n",
    "print(f\"Value: {study_base.best_value:.4f}\")\n",
    "print(\"Best hyperparameters:\", study_base.best_params)\n",
    "\n",
    "# Create study for LDA pipeline\n",
    "study_lda = optuna.create_study(direction=\"maximize\")\n",
    "objective_lda = generate_objective(X, y - 1, lda_pipeline, xgb_search_space)\n",
    "study_lda.optimize(objective_lda, n_trials=6)\n",
    "\n",
    "print(\"\\nBest trial (StandardScaler + LDA):\")\n",
    "print(f\"Value: {study_lda.best_value:.4f}\")\n",
    "print(\"Best hyperparameters:\", study_lda.best_params)\n",
    "\n",
    "# Train final models with best parameters\n",
    "# Original data pipeline\n",
    "best_base_model = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"classifier\", xgb.XGBClassifier(random_state=42, **study_base.best_params)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# LDA pipeline\n",
    "best_lda_model = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"lda\", LinearDiscriminantAnalysis()),\n",
    "        (\"classifier\", xgb.XGBClassifier(random_state=42, **study_lda.best_params)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Evaluate final models using cross-validation\n",
    "final_results = pd.DataFrame(\n",
    "    {\n",
    "        \"Model\": [\"XGBoost + StandardScaler (Optimized)\", \"XGBoost + LDA (Optimized)\"],\n",
    "        \"Accuracy\": [\n",
    "            np.mean(cross_val_score(best_base_model, X, y - 1, cv=5)),\n",
    "            np.mean(cross_val_score(best_lda_model, X, y - 1, cv=5)),\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\nFinal Results after Hyperparameter Optimization:\")\n",
    "print(final_results.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit models on full data to generate confusion matrices\n",
    "# Get the best models from optuna studies\n",
    "best_base_model = study_base.best_trial.user_attrs.get(\"model\", best_base_model)\n",
    "best_lda_model = study_lda.best_trial.user_attrs.get(\"model\", best_lda_model)\n",
    "\n",
    "# Generate predictions using the optimized models\n",
    "base_preds = best_base_model.predict(X)\n",
    "lda_preds = best_lda_model.predict(X)\n",
    "\n",
    "# Plot confusion matrices\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Confusion matrix for base model\n",
    "cm_base = confusion_matrix(y - 1, base_preds)\n",
    "sns.heatmap(cm_base, annot=True, fmt=\"d\", ax=ax1)\n",
    "ax1.set_title(\"Confusion Matrix - XGBoost + StandardScaler\")\n",
    "ax1.set_xlabel(\"Predicted\")\n",
    "ax1.set_ylabel(\"True\")\n",
    "\n",
    "# Confusion matrix for LDA model\n",
    "cm_lda = confusion_matrix(y - 1, lda_preds)\n",
    "sns.heatmap(cm_lda, annot=True, fmt=\"d\", ax=ax2)\n",
    "ax2.set_title(\"Confusion Matrix - XGBoost + LDA\")\n",
    "ax2.set_xlabel(\"Predicted\")\n",
    "ax2.set_ylabel(\"True\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "upd_ai_201",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

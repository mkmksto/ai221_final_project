# Final Todos for submission

## Code

- [x] VGG16 model as baseline

### ML models (raw, without hyperparam optimization)

- [ ] SVM
- [x] XGBoost
- [ ] Random Forest
- [ ] K-Nearest Neighbors
- [ ] Naive Bayes
- [ ] Decision Trees
- [ ] AdaBoost
- [ ] MLP
- [ ] autosklearn

### ML Models with dim reduction (e.g. Autoencoder)

Only perform on maybe the top 3 best models from the previous section

- [x] SVM + AE (https://github.com/mkmksto/ai221_final_project/blob/main/notebooks/cantor_official_classification.ipynb)
- [x] KNN + AE
- [x] MLP + AE
- [x] Logistic Regression + AE
- [x] Random Forest + AE (https://github.com/mkmksto/ai221_final_project/blob/main/notebooks/cantor_training_autoencoder.ipynb)
- [x] Decision Trees + AE
- [ ] XGBoost + PCA/LDA/AE

### Hyperparam optimized models

- [x] SVM + AE + Optuna
- [x] KNN + AE + Optuna
- [x] MLP + AE + Optuna
- [ ] XGBoost + AE + Optuna
